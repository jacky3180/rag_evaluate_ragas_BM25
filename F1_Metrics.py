"""
F1-scoreè®¡ç®—æ¨¡å—
åŸºäºBM25è¯„ä¼°çš„Precisionå’ŒRecallè®¡ç®—F1-scoreï¼ˆè°ƒå’Œå¹³å‡æ•°ï¼‰

åŠŸèƒ½ï¼š
1. è°ƒç”¨BM25_evaluate.pyä¸­çš„precisionå’Œrecallè®¡ç®—å‡½æ•°
2. è®¡ç®—F1-score = 2 * (precision * recall) / (precision + recall)
3. æä¾›F1-scoreè®¡ç®—æ¥å£
"""

import os
import numpy as np
from typing import Dict, Any, List, Optional
from config import debug_print, verbose_print, info_print, error_print, QUIET_MODE
from BM25_evaluate import BM25Evaluator
from read_chuck import EvaluationConfig


class F1ScoreCalculator:
    """F1-scoreè®¡ç®—å™¨"""
    
    def __init__(self, config: EvaluationConfig):
        """
        åˆå§‹åŒ–F1-scoreè®¡ç®—å™¨
        
        Args:
            config: è¯„ä¼°é…ç½®
        """
        self.config = config
        self.bm25_evaluator = BM25Evaluator(config)
    
    def calculate_f1_score(self, precision: float, recall: float) -> float:
        """
        è®¡ç®—F1-scoreï¼ˆPrecisionå’ŒRecallçš„è°ƒå’Œå¹³å‡æ•°ï¼‰
        
        Args:
            precision: ç²¾ç¡®ç‡
            recall: å¬å›ç‡
            
        Returns:
            float: F1-scoreå€¼
        """
        if precision is None or recall is None:
            return 0.0
        
        if precision + recall == 0:
            return 0.0
        
        f1_score = 2 * (precision * recall) / (precision + recall)
        return f1_score
    
    def calculate_f1_scores_from_bm25_results(self, bm25_results: Dict[str, Any]) -> Dict[str, Any]:
        """
        ä»BM25è¯„ä¼°ç»“æœä¸­è®¡ç®—F1-score
        
        Args:
            bm25_results: BM25è¯„ä¼°ç»“æœ
            
        Returns:
            Dict[str, Any]: åŒ…å«F1-scoreçš„ç»“æœ
        """
        info_print("ğŸ” å¼€å§‹è®¡ç®—F1-score...")
        
        # è·å–å¹³å‡Precisionå’ŒRecall
        avg_precision = bm25_results.get('avg_precision', 0)
        avg_recall = bm25_results.get('avg_recall', 0)
        
        # è®¡ç®—å¹³å‡F1-score
        avg_f1 = self.calculate_f1_score(avg_precision, avg_recall)
        
        # è®¡ç®—æ¯ä¸ªæ ·æœ¬çš„F1-score
        precision_scores = bm25_results.get('precision_scores', [])
        recall_scores = bm25_results.get('recall_scores', [])
        
        f1_scores = []
        for i in range(len(precision_scores)):
            precision = precision_scores[i] if i < len(precision_scores) else 0
            recall = recall_scores[i] if i < len(recall_scores) else 0
            f1_score = self.calculate_f1_score(precision, recall)
            f1_scores.append(f1_score)
        
        # æ„å»ºç»“æœ
        results = {
            'avg_f1': avg_f1,
            'f1_scores': f1_scores,
            'avg_precision': avg_precision,
            'avg_recall': avg_recall,
            'total_samples': len(f1_scores),
            'bm25_results': bm25_results  # ä¿ç•™åŸå§‹BM25ç»“æœ
        }
        
        info_print(f"âœ… F1-scoreè®¡ç®—å®Œæˆ")
        info_print(f"ğŸ“Š å¹³å‡F1-score: {avg_f1:.4f}")
        info_print(f"ğŸ“Š å¹³å‡Precision: {avg_precision:.4f}")
        info_print(f"ğŸ“Š å¹³å‡Recall: {avg_recall:.4f}")
        info_print(f"ğŸ“Š æ ·æœ¬æ•°: {len(f1_scores)}")
        
        return results
    
    def run_f1_evaluation(self) -> Dict[str, Any]:
        """
        è¿è¡Œå®Œæ•´çš„F1-scoreè¯„ä¼°
        
        Returns:
            Dict[str, Any]: F1-scoreè¯„ä¼°ç»“æœ
        """
        info_print("ğŸš€ å¼€å§‹F1-scoreè¯„ä¼°")
        info_print("=" * 60)
        
        try:
            # 1. è¿è¡ŒBM25è¯„ä¼°è·å–Precisionå’ŒRecall
            bm25_results = self.bm25_evaluator.run_evaluation()
            
            if "error" in bm25_results:
                return {"error": f"BM25è¯„ä¼°å¤±è´¥: {bm25_results['error']}"}
            
            # 2. åŸºäºBM25ç»“æœè®¡ç®—F1-score
            f1_results = self.calculate_f1_scores_from_bm25_results(bm25_results)
            
            return f1_results
            
        except Exception as e:
            error_msg = f"F1-scoreè¯„ä¼°å¤±è´¥: {str(e)}"
            info_print(f"âŒ {error_msg}")
            import traceback
            traceback.print_exc()
            return {"error": error_msg}


def calculate_f1_score(precision: float, recall: float) -> float:
    """
    è®¡ç®—F1-scoreçš„é™æ€å‡½æ•°
    
    Args:
        precision: ç²¾ç¡®ç‡
        recall: å¬å›ç‡
        
    Returns:
        float: F1-scoreå€¼
    """
    if precision is None or recall is None:
        return 0.0
    
    if precision + recall == 0:
        return 0.0
    
    f1_score = 2 * (precision * recall) / (precision + recall)
    return f1_score


def main():
    """ä¸»å‡½æ•°"""
    # åˆ›å»ºé…ç½®
    config = EvaluationConfig(
        api_key=os.getenv("QWEN_API_KEY", "dummy_key"),
        api_base=os.getenv("QWEN_API_BASE", "dummy_base"),
        excel_file_path=os.getenv("EXCEL_FILE_PATH", "standardDataset/standardDataset.xlsx")
    )
    
    # åˆ›å»ºF1-scoreè®¡ç®—å™¨å¹¶è¿è¡Œè¯„ä¼°
    calculator = F1ScoreCalculator(config)
    results = calculator.run_f1_evaluation()
    
    if "error" in results:
        info_print(f"âŒ F1-scoreè¯„ä¼°å¤±è´¥: {results['error']}")
    else:
        info_print(f"\nğŸ‰ F1-scoreè¯„ä¼°æˆåŠŸå®Œæˆï¼")
        info_print(f"ğŸ“Š æœ€ç»ˆF1-score: {results['avg_f1']:.4f}")


if __name__ == "__main__":
    main()
