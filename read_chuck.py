"""
æ–‡ä»¶è¯»å–ã€æ–‡æœ¬å¤„ç†å’Œåˆ†å—æ¨¡å—
ä»rag_evaluator.pyä¸­æå–çš„æ•°æ®å¤„ç†ç›¸å…³åŠŸèƒ½

åŒ…å«åŠŸèƒ½ï¼š
1. DataLoader - æ•°æ®åŠ è½½å’Œè§£æ
2. TextProcessor - æ–‡æœ¬å¤„ç†å’Œåˆ†å—
"""

import os
import json
import pandas as pd
import numpy as np
import re
from typing import Dict, List, Optional, Any, Tuple
from config import debug_print, verbose_print, info_print, error_print, QUIET_MODE
from dataclasses import dataclass

@dataclass
class EvaluationConfig:
    """è¯„ä¼°é…ç½®ç±»"""
    # APIé…ç½®
    api_key: str
    api_base: str
    model_name: str = "qwen-plus"
    embedding_model: str = "text-embedding-v1"
    
    # Ollamaé…ç½®
    use_ollama: bool = False
    ollama_url: str = "http://localhost:11434"
    ollama_embedding_model: str = "embeddinggemma:300m"
    ollama_llm_model: str = "llama3.2:3b"
    
    # è¯„ä¼°é…ç½®ï¼ˆLLM è¾“å‡ºç¨³å®šæ€§å‚æ•°ï¼‰
    temperature: float = 0.0  # ä½¿ç”¨ 0.0 ä»¥è·å¾—æ›´ç¨³å®šçš„è¾“å‡ºï¼Œæé«˜ Ragas è§£æå™¨æˆåŠŸç‡
    top_p: float = 0.1  # é™ä½é‡‡æ ·å¤šæ ·æ€§ï¼Œåªä»æœ€é«˜æ¦‚ç‡çš„ 10% token ä¸­é€‰æ‹©
    max_tokens: int = 2000  # æœ€å¤§ç”Ÿæˆ token æ•°
    max_chunk_length: int = 200
    
    # æ€§èƒ½ä¼˜åŒ–å‚æ•°
    max_workers: int = 16  # æœ€å¤§å¹¶å‘å·¥ä½œçº¿ç¨‹æ•°ï¼Œæå‡è¯„ä¼°é€Ÿåº¦
    batch_size: int = 10  # æ‰¹å¤„ç†å¤§å°ï¼Œå‡å°‘ API è°ƒç”¨æ¬¡æ•°
    
    # æ–‡ä»¶é…ç½®
    excel_file_path: str = None
    required_columns: List[str] = None
    
    def __post_init__(self):
        if self.required_columns is None:
            self.required_columns = [
                'user_input', 
                'retrieved_contexts', 
                'response', 
                'reference_contexts', 
                'reference'
            ]
        if self.excel_file_path is None:
            self.excel_file_path = os.getenv("EXCEL_FILE_PATH", "standardDataset/standardDataset.xlsx")

class DataLoader:
    """æ•°æ®åŠ è½½å’Œè§£ææ¨¡å—"""
    
    def __init__(self, config: EvaluationConfig):
        self.config = config
    
    def load_excel_data(self) -> Optional[pd.DataFrame]:
        """
        ä»Excelæ–‡ä»¶åŠ è½½æ•°æ®
        
        Returns:
            pd.DataFrame: åŠ è½½çš„æ•°æ®ï¼Œå¤±è´¥æ—¶è¿”å›None
        """
        info_print(f"ğŸ“– è¯»å–Excelæ–‡ä»¶: {self.config.excel_file_path}")
        
        if not os.path.exists(self.config.excel_file_path):
            info_print(f"âŒ Excelæ–‡ä»¶ä¸å­˜åœ¨: {self.config.excel_file_path}")
            return None
        
        try:
            # åªè¯»å–æŒ‡å®šçš„åˆ—
            df = pd.read_excel(
                self.config.excel_file_path, 
                usecols=self.config.required_columns
            )
            info_print(f"âœ… æˆåŠŸè¯»å– {len(df)} è¡Œæ•°æ®")
            info_print(f"ğŸ“‹ åˆ—å: {list(df.columns)}")
            return df
        except Exception as e:
            info_print(f"âŒ è¯»å–Excelæ–‡ä»¶å¤±è´¥: {e}")
            return None
    
    def validate_data(self, df: pd.DataFrame) -> bool:
        """
        éªŒè¯æ•°æ®å®Œæ•´æ€§
        
        Args:
            df: æ•°æ®DataFrame
            
        Returns:
            bool: éªŒè¯æ˜¯å¦é€šè¿‡
        """
        missing_fields = [
            field for field in self.config.required_columns 
            if field not in df.columns
        ]
        
        if missing_fields:
            info_print(f"âŒ Excelæ–‡ä»¶ç¼ºå°‘å¿…è¦å­—æ®µ: {missing_fields}")
            info_print(f"å½“å‰å­—æ®µ: {list(df.columns)}")
            return False
        
        info_print("âœ… æ•°æ®éªŒè¯é€šè¿‡")
        return True

class TextProcessor:
    """æ–‡æœ¬å¤„ç†å’Œåˆ†å—æ¨¡å—"""
    
    def __init__(self, config: EvaluationConfig):
        self.config = config
    
    def split_text_into_chunks(self, text: str, max_chunk_length: Optional[int] = None) -> List[str]:
        """
        å°†æ–‡æœ¬åˆ†å‰²æˆå°å—æ®µè½ï¼ŒæŒ‰ç©ºè¡Œåˆ†å‰²
        
        Args:
            text: è¦åˆ†å‰²çš„æ–‡æœ¬
            max_chunk_length: æ¯ä¸ªå—çš„æœ€å¤§é•¿åº¦ï¼ˆä¿ç•™å‚æ•°ä»¥å…¼å®¹æ€§ï¼‰
            
        Returns:
            List[str]: åˆ†å‰²åçš„æ–‡æœ¬å—åˆ—è¡¨
        """
        if not text or not text.strip():
            return [text] if text else []
        
        # æŒ‰ç©ºè¡Œåˆ†å‰²ï¼ˆåŒæ¢è¡Œç¬¦æˆ–æ›´å¤šæ¢è¡Œç¬¦ï¼‰
        # ä½¿ç”¨æ­£åˆ™è¡¨è¾¾å¼åŒ¹é…ä¸€ä¸ªæˆ–å¤šä¸ªè¿ç»­çš„æ¢è¡Œç¬¦
        chunks = re.split(r'\n\s*\n', text.strip())
        
        # è¿‡æ»¤æ‰ç©ºå—
        chunks = [chunk.strip() for chunk in chunks if chunk.strip()]
        
        # å¦‚æœæ²¡æœ‰æ‰¾åˆ°ç©ºè¡Œåˆ†å‰²ï¼Œè¿”å›æ•´ä¸ªæ–‡æœ¬ä½œä¸ºä¸€ä¸ªå—
        if not chunks:
            chunks = [text.strip()]
        
        return chunks
    
    def process_contexts(self, contexts_str: Any) -> List[str]:
        """
        å¤„ç†ä¸Šä¸‹æ–‡å­—æ®µï¼Œç¡®ä¿æ˜¯åˆ—è¡¨æ ¼å¼
        
        Args:
            contexts_str: ä¸Šä¸‹æ–‡å­—ç¬¦ä¸²æˆ–åˆ—è¡¨
            
        Returns:
            List[str]: å¤„ç†åçš„ä¸Šä¸‹æ–‡åˆ—è¡¨
        """
        try:
            # å¤„ç†pandas Seriesæˆ–numpyæ•°ç»„
            if hasattr(contexts_str, '__iter__') and not isinstance(contexts_str, (str, list)):
                if hasattr(contexts_str, 'iloc'):
                    contexts_str = contexts_str.iloc[0]
                elif hasattr(contexts_str, '__len__') and len(contexts_str) > 0:
                    contexts_str = contexts_str[0]
                else:
                    return []
            
            # æ£€æŸ¥æ˜¯å¦ä¸ºnumpyæ•°ç»„
            if hasattr(contexts_str, 'dtype'):
                contexts_str = str(contexts_str)
            
            # å®‰å…¨æ£€æŸ¥æ˜¯å¦ä¸ºNaNæˆ–ç©º
            if contexts_str is None:
                return []
            
            try:
                if pd.isna(contexts_str):
                    return []
            except (TypeError, ValueError):
                pass
            
            if (isinstance(contexts_str, str) and contexts_str.strip() == '') or \
               (isinstance(contexts_str, list) and len(contexts_str) == 0) or \
               (isinstance(contexts_str, list) and all(
                   (isinstance(item, str) and item.strip() == '') or 
                   (hasattr(item, '__iter__') and not isinstance(item, str) and len(str(item).strip()) == 0)
                   for item in contexts_str)):
                return []
            
            if isinstance(contexts_str, str):
                # å¦‚æœæ˜¯å­—ç¬¦ä¸²ï¼Œå°è¯•è§£æä¸ºåˆ—è¡¨
                try:
                    # å¦‚æœæ˜¯JSONæ ¼å¼çš„å­—ç¬¦ä¸²
                    parsed = json.loads(contexts_str)
                    if isinstance(parsed, list):
                        return [str(item) for item in parsed if str(item).strip()]
                    else:
                        return [str(parsed)]
                except:
                    # å¦‚æœä¸æ˜¯JSONï¼Œä½¿ç”¨åˆ†å—é€»è¾‘æŒ‰ç©ºè¡Œåˆ†å‰²
                    parts = self.split_text_into_chunks(contexts_str)
                    return parts
            elif isinstance(contexts_str, list):
                return [str(item) for item in contexts_str if str(item).strip()]
            else:
                return [str(contexts_str)] if str(contexts_str).strip() else []
        except Exception as e:
            info_print(f"âš ï¸ å¤„ç†ä¸Šä¸‹æ–‡æ—¶å‡ºé”™: {e}")
            return []
    
    def parse_context_columns(self, df: pd.DataFrame) -> pd.DataFrame:
        """
        è§£æä¸Šä¸‹æ–‡åˆ—ï¼Œå°†å­—ç¬¦ä¸²è½¬æ¢ä¸ºåˆ—è¡¨æ ¼å¼
        
        Args:
            df: åŒ…å«ä¸Šä¸‹æ–‡æ•°æ®çš„DataFrame
            
        Returns:
            pd.DataFrame: å¤„ç†åçš„DataFrame
        """
        info_print("ğŸ”§ è§£æä¸Šä¸‹æ–‡åˆ—...")
        
        # å¤„ç†retrieved_contextså’Œreference_contextså­—æ®µ
        df['retrieved_contexts'] = df['retrieved_contexts'].apply(self.process_contexts)
        df['reference_contexts'] = df['reference_contexts'].apply(self.process_contexts)
        
        return df
    
    def is_empty_row_data(self, retrieved_contexts: List[str], reference_contexts: List[str], 
                         user_input: str, response: str) -> bool:
        """
        æ£€æŸ¥æ˜¯å¦ä¸ºç©ºè¡Œæ•°æ®
        
        Args:
            retrieved_contexts: æ£€ç´¢ä¸Šä¸‹æ–‡åˆ—è¡¨
            reference_contexts: å‚è€ƒä¸Šä¸‹æ–‡åˆ—è¡¨
            user_input: ç”¨æˆ·è¾“å…¥
            response: å›ç­”
            
        Returns:
            bool: æ˜¯å¦ä¸ºç©ºè¡Œæ•°æ®
        """
        return (
            not retrieved_contexts or 
            not reference_contexts or 
            not user_input or 
            not response or
            (isinstance(retrieved_contexts, list) and len(retrieved_contexts) == 0) or
            (isinstance(reference_contexts, list) and len(reference_contexts) == 0) or
            (isinstance(user_input, str) and user_input.strip() == '') or
            (isinstance(response, str) and response.strip() == '') or
            pd.isna(user_input) or
            pd.isna(response) or
            (isinstance(retrieved_contexts, list) and all(
                pd.isna(item) or (isinstance(item, str) and item.strip() == '') 
                for item in retrieved_contexts)) or
            (isinstance(reference_contexts, list) and all(
                pd.isna(item) or (isinstance(item, str) and item.strip() == '') 
                for item in reference_contexts))
        )
