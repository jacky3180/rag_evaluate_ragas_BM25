"""
RAGè¯„ä¼°ç³»ç»ŸWebåº”ç”¨
åŸºäºFastAPI + HTML/CSS/JSå®ç°
"""

from fastapi import FastAPI, HTTPException, UploadFile, File
from fastapi.staticfiles import StaticFiles
from fastapi.responses import HTMLResponse, FileResponse
from fastapi.middleware.cors import CORSMiddleware
from pydantic import BaseModel
import asyncio
import os
import json
import pandas as pd
import re
from typing import Dict, Any, List, Optional
from config import debug_print, verbose_print, info_print, error_print, QUIET_MODE
import traceback
import tempfile
from pathlib import Path
from uploadFile import upload_document, get_upload_info, delete_uploaded_file, upload_knowledge_document, get_knowledge_documents, delete_knowledge_document, get_dataset_files
from standardDatasetBuild import build_standard_dataset
from env_manager import update_env_file, get_env_value

# å¯¼å…¥ç¼“å­˜æ¨¡å—
from api_cache import (
    get_history_cache, get_stats_cache, get_eval_cache,
    clear_all_caches, get_all_cache_stats, cache_response
)

# å¯¼å…¥è¯„ä¼°æ¨¡å—
from BM25_evaluate import BM25Evaluator
from rag_evaluator import MainController, RagasMetricsConfig
from read_chuck import EvaluationConfig
from MRR_Metrics import MRREvaluator
from MAP_Metrics import MAPEvaluator
from NDCG_Metrics import NDCGEvaluator
from F1_Metrics import F1ScoreCalculator

def calculate_text_similarity(text1: str, text2: str) -> float:
    """
    è®¡ç®—ä¸¤ä¸ªæ–‡æœ¬çš„ç›¸ä¼¼åº¦
    ä½¿ç”¨Jaccardç›¸ä¼¼åº¦å’Œå­—ç¬¦é‡å åº¦
    """
    if not text1 or not text2:
        return 0.0
    
    # æ¸…ç†æ–‡æœ¬ï¼šç§»é™¤æ ‡ç‚¹ç¬¦å·ï¼Œè½¬æ¢ä¸ºå°å†™
    def clean_text(text):
        # ç§»é™¤æ ‡ç‚¹ç¬¦å·å’Œç‰¹æ®Šå­—ç¬¦ï¼Œä¿ç•™ä¸­æ–‡ã€è‹±æ–‡ã€æ•°å­—
        cleaned = re.sub(r'[^\w\s\u4e00-\u9fff]', ' ', text)
        # ç§»é™¤å¤šä½™ç©ºæ ¼
        cleaned = re.sub(r'\s+', ' ', cleaned).strip()
        return cleaned.lower()
    
    clean_text1 = clean_text(text1)
    clean_text2 = clean_text(text2)
    
    # 1. Jaccardç›¸ä¼¼åº¦ï¼ˆåŸºäºè¯ï¼‰
    words1 = set(clean_text1.split())
    words2 = set(clean_text2.split())
    
    if not words1 or not words2:
        return 0.0
    
    intersection = len(words1.intersection(words2))
    union = len(words1.union(words2))
    jaccard_similarity = intersection / union if union > 0 else 0.0
    
    # 2. å­—ç¬¦é‡å åº¦ï¼ˆåŸºäºå­—ç¬¦ï¼‰
    chars1 = set(clean_text1)
    chars2 = set(clean_text2)
    char_intersection = len(chars1.intersection(chars2))
    char_union = len(chars1.union(chars2))
    char_similarity = char_intersection / char_union if char_union > 0 else 0.0
    
    # 3. å­å­—ç¬¦ä¸²åŒ¹é…åº¦
    substring_similarity = 0.0
    if len(clean_text1) > 10 and len(clean_text2) > 10:
        # æ£€æŸ¥è¾ƒçŸ­çš„æ–‡æœ¬æ˜¯å¦åŒ…å«åœ¨è¾ƒé•¿çš„æ–‡æœ¬ä¸­
        shorter, longer = (clean_text1, clean_text2) if len(clean_text1) < len(clean_text2) else (clean_text2, clean_text1)
        if shorter in longer:
            substring_similarity = len(shorter) / len(longer)
        else:
            # æ£€æŸ¥éƒ¨åˆ†åŒ¹é…
            max_match = 0
            for i in range(len(shorter) - 5):  # è‡³å°‘5ä¸ªå­—ç¬¦çš„åŒ¹é…
                for j in range(i + 5, len(shorter) + 1):
                    substring = shorter[i:j]
                    if substring in longer:
                        max_match = max(max_match, len(substring))
            substring_similarity = max_match / len(longer) if longer else 0.0
    
    # ç»¼åˆç›¸ä¼¼åº¦ï¼šåŠ æƒå¹³å‡
    final_similarity = (
        jaccard_similarity * 0.4 +      # è¯çº§åˆ«ç›¸ä¼¼åº¦æƒé‡40%
        char_similarity * 0.3 +         # å­—ç¬¦çº§åˆ«ç›¸ä¼¼åº¦æƒé‡30%
        substring_similarity * 0.3      # å­å­—ç¬¦ä¸²åŒ¹é…åº¦æƒé‡30%
    )
    
    # ä¼˜åŒ–ï¼šå¦‚æœçŸ­æ–‡æœ¬å®Œå…¨åŒ…å«åœ¨é•¿æ–‡æœ¬ä¸­ï¼Œç»™äºˆæ›´é«˜çš„ç›¸ä¼¼åº¦
    # è¿™æ˜¯ä¸ºäº†å¤„ç†"æ£€ç´¢åˆ†å—åŒ…å«æ ‡å‡†ç­”æ¡ˆåˆ†å—"çš„åœºæ™¯
    if len(clean_text1) > len(clean_text2):
        # text1æ˜¯é•¿æ–‡æœ¬ï¼Œtext2æ˜¯çŸ­æ–‡æœ¬
        if clean_text2 in clean_text1:
            # çŸ­æ–‡æœ¬å®Œå…¨åŒ…å«åœ¨é•¿æ–‡æœ¬ä¸­ï¼Œç»™äºˆåŒ…å«åº¦å¥–åŠ±
            # ä½¿ç”¨æ›´é«˜çš„ç›¸ä¼¼åº¦åˆ†æ•°ï¼Œç¡®ä¿èƒ½é€šè¿‡é˜ˆå€¼
            containment_similarity = 0.8  # å›ºå®šç»™äºˆ0.8çš„ç›¸ä¼¼åº¦
            final_similarity = max(final_similarity, containment_similarity)
    elif len(clean_text2) > len(clean_text1):
        # text2æ˜¯é•¿æ–‡æœ¬ï¼Œtext1æ˜¯çŸ­æ–‡æœ¬
        if clean_text1 in clean_text2:
            # çŸ­æ–‡æœ¬å®Œå…¨åŒ…å«åœ¨é•¿æ–‡æœ¬ä¸­ï¼Œç»™äºˆåŒ…å«åº¦å¥–åŠ±
            # ä½¿ç”¨æ›´é«˜çš„ç›¸ä¼¼åº¦åˆ†æ•°ï¼Œç¡®ä¿èƒ½é€šè¿‡é˜ˆå€¼
            containment_similarity = 0.8  # å›ºå®šç»™äºˆ0.8çš„ç›¸ä¼¼åº¦
            final_similarity = max(final_similarity, containment_similarity)
    
    # æ–°å¢ï¼šæ£€æŸ¥è¯­ä¹‰åŒ…å«åº¦æ˜¯å¦è¶…è¿‡é˜ˆå€¼ï¼ˆå®Œæ•´åŒ…å«æ£€æµ‹ï¼‰
    semantic_containment_threshold = float(os.getenv("SEMANTIC_CONTAINMENT_THRESHOLD", "0.9"))
    
    # è®¡ç®—è¯­ä¹‰åŒ…å«åº¦ï¼ˆåŸºäºè¯çº§åˆ«çš„é‡å ï¼‰
    if len(words1) > 0 and len(words2) > 0:
        # è®¡ç®—è¾ƒçŸ­æ–‡æœ¬çš„è¯åœ¨è¾ƒé•¿æ–‡æœ¬ä¸­çš„åŒ…å«åº¦
        if len(words1) <= len(words2):
            # words1æ˜¯è¾ƒçŸ­çš„ï¼Œè®¡ç®—words1åœ¨words2ä¸­çš„åŒ…å«åº¦
            contained_words = words1.intersection(words2)
            semantic_containment = len(contained_words) / len(words1)
        else:
            # words2æ˜¯è¾ƒçŸ­çš„ï¼Œè®¡ç®—words2åœ¨words1ä¸­çš„åŒ…å«åº¦
            contained_words = words2.intersection(words1)
            semantic_containment = len(contained_words) / len(words2)
        
        # å¦‚æœè¯­ä¹‰åŒ…å«åº¦è¶…è¿‡é˜ˆå€¼ï¼Œç»™äºˆé«˜ç›¸ä¼¼åº¦åˆ†æ•°
        if semantic_containment >= semantic_containment_threshold:
            # ä½¿ç”¨åŒ…å«åº¦ä½œä¸ºç›¸ä¼¼åº¦åˆ†æ•°ï¼Œç¡®ä¿èƒ½é€šè¿‡é˜ˆå€¼
            containment_similarity = min(semantic_containment, 0.95)  # æœ€é«˜0.95ï¼Œé¿å…å®Œå…¨åŒ¹é…
            final_similarity = max(final_similarity, containment_similarity)
    
    return min(final_similarity, 1.0)

# å¯¼å…¥æ•°æ®åº“æ¨¡å—
from database.db_service import DatabaseService
from database.db_config import create_tables, test_connection
db = DatabaseService()
init_database = create_tables

app = FastAPI(title="RAGè¯„ä¼°ç³»ç»Ÿ", description="BM25å’ŒRagasè¯„ä¼°ç³»ç»ŸWebç•Œé¢")

# æ·»åŠ CORSä¸­é—´ä»¶
app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

# æŒ‚è½½é™æ€æ–‡ä»¶
app.mount("/static", StaticFiles(directory="static"), name="static")

# æ·»åŠ æ„å»ºæ•°æ®é›†é¡µé¢çš„ç›´æ¥è·¯ç”±
@app.get("/standardDataset_build.html")
async def build_dataset_page():
    """æ„å»ºæ•°æ®é›†é¡µé¢"""
    with open("static/standardDataset_build.html", "r", encoding="utf-8") as f:
        content = f.read()
    return HTMLResponse(content=content)

# å…¨å±€å˜é‡å­˜å‚¨è¯„ä¼°ç»“æœ
bm25_results = None
ragas_results = {}

class EvaluationRequest(BaseModel):
    """è¯„ä¼°è¯·æ±‚æ¨¡å‹"""
    dataset_file: Optional[str] = "standardDataset.xlsx"

class EvaluationResponse(BaseModel):
    success: bool
    message: str
    data: Dict[str, Any] = {}

class SaveEvaluationRequest(BaseModel):
    """ä¿å­˜è¯„ä¼°ç»“æœè¯·æ±‚æ¨¡å‹"""
    evaluation_type: str  # "BM25" æˆ– "RAGAS"
    description: str = ""

class BM25CombinedResults(BaseModel):
    """BM25åˆå¹¶ç»“æœæ¨¡å‹"""
    context_precision: float
    context_recall: float
    f1_score: float
    mrr: float
    map: float
    ndcg: float
    total_samples: int
    irrelevant_chunks: int
    missed_chunks: int
    relevant_chunks: int
    description: str = ""

class SaveEvaluationResponse(BaseModel):
    """ä¿å­˜è¯„ä¼°ç»“æœå“åº”æ¨¡å‹"""
    success: bool
    message: str
    evaluation_id: Optional[int] = None


@app.get("/", response_class=HTMLResponse)
async def read_root():
    """ä¸»é¡µé¢"""
    try:
        with open("static/index.html", "r", encoding="utf-8") as f:
            return HTMLResponse(content=f.read())
    except UnicodeDecodeError:
        # å¦‚æœUTF-8è§£ç å¤±è´¥ï¼Œå°è¯•å…¶ä»–ç¼–ç 
        try:
            with open("static/index.html", "r", encoding="gbk") as f:
                return HTMLResponse(content=f.read())
        except:
            # å¦‚æœéƒ½å¤±è´¥ï¼Œè¿”å›ç®€å•çš„HTMLé¡µé¢
            return HTMLResponse(content="""
            <!DOCTYPE html>
            <html>
            <head>
                <title>RAGè¯„ä¼°ç³»ç»Ÿ</title>
                <meta charset="utf-8">
            </head>
            <body>
                <h1>RAGè¯„ä¼°ç³»ç»Ÿ</h1>
                <p>ç³»ç»Ÿæ­£åœ¨å¯åŠ¨ä¸­ï¼Œè¯·ç¨ååˆ·æ–°é¡µé¢...</p>
                <script>setTimeout(() => location.reload(), 2000);</script>
            </body>
            </html>
            """)

@app.post("/api/mrr/evaluate", response_model=EvaluationResponse)
async def run_mrr_evaluation(request: Optional[EvaluationRequest] = None):
    """è¿è¡ŒMRRè¯„ä¼°"""
    try:
        # è·å–æ•°æ®é›†æ–‡ä»¶è·¯å¾„
        dataset_file = "standardDataset.xlsx" if request is None else request.dataset_file
        excel_file_path = f"standardDataset/{dataset_file}"
        
        # è®¾ç½®ç¯å¢ƒå˜é‡
        os.environ["EXCEL_FILE_PATH"] = excel_file_path
        os.environ.setdefault("SIMILARITY_THRESHOLD", "0.5")
        
        # åˆ›å»ºæœ€å°åŒ–é…ç½®
        from read_chuck import EvaluationConfig
        config = EvaluationConfig(
            api_key="dummy",
            api_base="dummy",
            excel_file_path=os.getenv("EXCEL_FILE_PATH", "standardDataset/standardDataset.xlsx")
        )
        
        # åˆ›å»ºMRRè¯„ä¼°å™¨å¹¶è¿è¡Œè¯„ä¼°
        evaluator = MRREvaluator(config)
        results = evaluator.run_evaluation()
        
        if "error" in results:
            return EvaluationResponse(
                success=False,
                message=f"MRRè¯„ä¼°å¤±è´¥: {results['error']}"
            )
        
        # æ ¼å¼åŒ–ç»“æœ
        formatted_results = {
            "mrr": results.get("mrr", 0),
            "total_queries": results.get("total_queries", 0),
            "queries_with_relevant_chunks": results.get("queries_with_relevant_chunks", 0),
            "queries_without_relevant_chunks": results.get("queries_without_relevant_chunks", 0),
            "detailed_results": results.get("detailed_results", [])
        }
        
        return EvaluationResponse(
            success=True,
            message="MRRè¯„ä¼°å®Œæˆ",
            data=formatted_results
        )
        
    except Exception as e:
        error_msg = f"MRRè¯„ä¼°å¼‚å¸¸: {str(e)}"
        info_print(error_msg)
        traceback.print_exc()
        return EvaluationResponse(
            success=False,
            message=error_msg
        )

@app.post("/api/map/evaluate", response_model=EvaluationResponse)
async def run_map_evaluation(request: Optional[EvaluationRequest] = None):
    """è¿è¡ŒMAPè¯„ä¼°"""
    try:
        # è·å–æ•°æ®é›†æ–‡ä»¶è·¯å¾„
        dataset_file = "standardDataset.xlsx" if request is None else request.dataset_file
        excel_file_path = f"standardDataset/{dataset_file}"
        
        # è®¾ç½®ç¯å¢ƒå˜é‡
        os.environ["EXCEL_FILE_PATH"] = excel_file_path
        os.environ.setdefault("SIMILARITY_THRESHOLD", "0.5")
        
        # åˆ›å»ºæœ€å°åŒ–é…ç½®
        from read_chuck import EvaluationConfig
        config = EvaluationConfig(
            api_key="dummy",
            api_base="dummy",
            excel_file_path=os.getenv("EXCEL_FILE_PATH", "standardDataset/standardDataset.xlsx")
        )
        
        # åˆ›å»ºMAPè¯„ä¼°å™¨å¹¶è¿è¡Œè¯„ä¼°
        evaluator = MAPEvaluator(config)
        results = evaluator.run_evaluation()
        
        if "error" in results:
            return EvaluationResponse(
                success=False,
                message=f"MAPè¯„ä¼°å¤±è´¥: {results['error']}"
            )
        
        # æ ¼å¼åŒ–ç»“æœ
        formatted_results = {
            "map": results.get("map", 0),
            "total_queries": results.get("total_queries", 0),
            "queries_with_relevant_chunks": results.get("queries_with_relevant_chunks", 0),
            "queries_without_relevant_chunks": results.get("queries_without_relevant_chunks", 0),
            "detailed_results": results.get("detailed_results", [])
        }
        
        return EvaluationResponse(
            success=True,
            message="MAPè¯„ä¼°å®Œæˆ",
            data=formatted_results
        )
        
    except Exception as e:
        error_msg = f"MAPè¯„ä¼°å¼‚å¸¸: {str(e)}"
        info_print(error_msg)
        traceback.print_exc()
        return EvaluationResponse(
            success=False,
            message=error_msg
        )

@app.post("/api/ndcg/evaluate", response_model=EvaluationResponse)
async def run_ndcg_evaluation(request: Optional[EvaluationRequest] = None):
    """è¿è¡ŒNDCGè¯„ä¼°"""
    try:
        # è·å–æ•°æ®é›†æ–‡ä»¶è·¯å¾„
        dataset_file = "standardDataset.xlsx" if request is None else request.dataset_file
        excel_file_path = f"standardDataset/{dataset_file}"
        
        # è®¾ç½®ç¯å¢ƒå˜é‡
        os.environ["EXCEL_FILE_PATH"] = excel_file_path
        os.environ.setdefault("SIMILARITY_THRESHOLD", "0.5")
        
        # åˆ›å»ºæœ€å°åŒ–é…ç½®
        from read_chuck import EvaluationConfig
        config = EvaluationConfig(
            api_key="dummy",
            api_base="dummy",
            excel_file_path=os.getenv("EXCEL_FILE_PATH", "standardDataset/standardDataset.xlsx")
        )
        
        # åˆ›å»ºNDCGè¯„ä¼°å™¨å¹¶è¿è¡Œè¯„ä¼°
        evaluator = NDCGEvaluator(config)
        results = evaluator.run_evaluation()
        
        if "error" in results:
            return EvaluationResponse(
                success=False,
                message=f"NDCGè¯„ä¼°å¤±è´¥: {results['error']}"
            )
        
        # æ ¼å¼åŒ–ç»“æœ
        formatted_results = {
            "ndcg": results.get("avg_ndcg", 0),
            "total_queries": results.get("total_queries", 0),
            "queries_with_relevant_chunks": results.get("queries_with_relevant_chunks", 0),
            "queries_without_relevant_chunks": results.get("queries_without_relevant_chunks", 0),
            "detailed_results": results.get("detailed_results", [])
        }
        
        return EvaluationResponse(
            success=True,
            message="NDCGè¯„ä¼°å®Œæˆ",
            data=formatted_results
        )
        
    except Exception as e:
        error_msg = f"NDCGè¯„ä¼°å¼‚å¸¸: {str(e)}"
        info_print(error_msg)
        traceback.print_exc()
        return EvaluationResponse(
            success=False,
            message=error_msg
        )

@app.post("/api/bm25/evaluate", response_model=EvaluationResponse)
async def run_bm25_evaluation(request: Optional[EvaluationRequest] = None):
    """è¿è¡ŒBM25è¯„ä¼°"""
    global bm25_results
    
    try:
        # è·å–æ•°æ®é›†æ–‡ä»¶è·¯å¾„
        dataset_file = "standardDataset.xlsx" if request is None else request.dataset_file
        excel_file_path = f"standardDataset/{dataset_file}"
        
        # åˆ›å»ºé…ç½®
        config = EvaluationConfig(
            api_key=os.getenv("QWEN_API_KEY", "dummy_key"),
            api_base=os.getenv("QWEN_API_BASE", "dummy_base"),
            excel_file_path=excel_file_path
        )
        
        # åˆ›å»ºè¯„ä¼°å™¨å¹¶è¿è¡Œè¯„ä¼°
        evaluator = BM25Evaluator(config)
        results = evaluator.run_evaluation()
        
        if "error" in results:
            return EvaluationResponse(
                success=False,
                message=f"BM25è¯„ä¼°å¤±è´¥: {results['error']}"
            )
        
        # è®¡ç®—F1-score
        f1_calculator = F1ScoreCalculator(config)
        f1_results = f1_calculator.calculate_f1_scores_from_bm25_results(results)
        
        # æ ¼å¼åŒ–ç»“æœï¼ˆåŒ…å«BM25ç›¸å…³æŒ‡æ ‡å’ŒF1-scoreï¼‰
        formatted_results = {
            "context_recall": results.get("avg_recall", 0),
            "context_precision": results.get("avg_precision", 0),
            "f1_score": f1_results.get("avg_f1", 0),
            "mrr": 0,  # å°†åœ¨JavaScriptä¸­ä»MRR APIè·å–
            "map": 0,  # å°†åœ¨JavaScriptä¸­ä»MAP APIè·å–
            "ndcg": 0,  # å°†åœ¨JavaScriptä¸­ä»NDCG APIè·å–
            "irrelevant_chunks": len(results.get("irrelevant_chunks", [])),
            "missed_chunks": len(results.get("missed_chunks", [])),
            "relevant_chunks": len(results.get("relevant_chunks", [])),
            "detailed_results": results.get("detailed_results", []),
            "total_samples": len(results.get("precision_scores", [])),
            "precision_scores": results.get("precision_scores", []),
            "recall_scores": results.get("recall_scores", []),
            "f1_scores": f1_results.get("f1_scores", [])
        }
        
        bm25_results = results
        info_print(f"ğŸ” è°ƒè¯•: è®¾ç½®bm25_resultså…¨å±€å˜é‡ï¼ŒåŒ…å«{len(results.get('detailed_results', []))}ä¸ªè¯¦ç»†ç»“æœ")
        
        return EvaluationResponse(
            success=True,
            message="BM25è¯„ä¼°å®Œæˆ",
            data=formatted_results
        )
        
    except Exception as e:
        error_msg = f"BM25è¯„ä¼°å¼‚å¸¸: {str(e)}"
        info_print(error_msg)
        traceback.print_exc()
        return EvaluationResponse(
            success=False,
            message=error_msg
        )

@app.get("/api/ragas/config", response_model=EvaluationResponse)
async def get_ragas_config():
    """è·å–Ragasè¯„ä¼°æŒ‡æ ‡é…ç½®"""
    try:
        config = RagasMetricsConfig.load()
        return EvaluationResponse(
            success=True,
            message="è·å–é…ç½®æˆåŠŸ",
            data={
                "enabled_metrics": config.enabled_metrics
            }
        )
    except Exception as e:
        error_msg = f"è·å–Ragasé…ç½®å¤±è´¥: {str(e)}"
        info_print(error_msg)
        return EvaluationResponse(
            success=False,
            message=error_msg
        )

@app.post("/api/ragas/config", response_model=EvaluationResponse)
async def save_ragas_config(request: dict):
    """ä¿å­˜Ragasè¯„ä¼°æŒ‡æ ‡é…ç½®"""
    try:
        enabled_metrics = request.get("enabled_metrics", [])
        
        # éªŒè¯å¿…é€‰æŒ‡æ ‡
        required = ['context_recall', 'context_precision']
        for metric in required:
            if metric not in enabled_metrics:
                return EvaluationResponse(
                    success=False,
                    message=f"å¿…é€‰æŒ‡æ ‡ {metric} ä¸èƒ½å–æ¶ˆ"
                )
        
        # ä¿å­˜é…ç½®
        config = RagasMetricsConfig(enabled_metrics=enabled_metrics)
        config.save()
        
        info_print(f"âœ… Ragasé…ç½®å·²ä¿å­˜: {len(enabled_metrics)} ä¸ªæŒ‡æ ‡")
        
        return EvaluationResponse(
            success=True,
            message=f"é…ç½®å·²ä¿å­˜ï¼Œå·²é€‰æ‹© {len(enabled_metrics)} ä¸ªæŒ‡æ ‡",
            data={
                "enabled_metrics": enabled_metrics
            }
        )
    except Exception as e:
        error_msg = f"ä¿å­˜Ragasé…ç½®å¤±è´¥: {str(e)}"
        info_print(error_msg)
        traceback.print_exc()
        return EvaluationResponse(
            success=False,
            message=error_msg
        )

@app.post("/api/ragas/evaluate", response_model=EvaluationResponse)
async def run_ragas_evaluation(request: Optional[EvaluationRequest] = None):
    """è¿è¡ŒRagasè¯„ä¼°"""
    global ragas_results
    
    try:
        # è·å–æ•°æ®é›†æ–‡ä»¶è·¯å¾„
        dataset_file = "standardDataset.xlsx" if request is None else request.dataset_file
        excel_file_path = f"standardDataset/{dataset_file}"
        
        # åˆ›å»ºé…ç½®
        config = EvaluationConfig(
            api_key=os.getenv("QWEN_API_KEY"),
            api_base=os.getenv("QWEN_API_BASE"),
            model_name=os.getenv("QWEN_MODEL_NAME", "qwen-plus"),
            embedding_model=os.getenv("QWEN_EMBEDDING_MODEL", "text-embedding-v1"),
            use_ollama=os.getenv("USE_OLLAMA", "false").lower() == "true",
            ollama_url=os.getenv("OLLAMA_URL", "http://localhost:11434"),
            ollama_embedding_model=os.getenv("OLLAMA_EMBEDDING_MODEL", "embeddinggemma:300m"),
            ollama_llm_model=os.getenv("OLLAMA_LLM_MODEL", "qwen2.5:7b"),
            excel_file_path=excel_file_path
        )
        
        # éªŒè¯é…ç½®
        if not config.api_key or not config.api_base:
            return EvaluationResponse(
                success=False,
                message="è¯·è®¾ç½®QWEN_API_KEYå’ŒQWEN_API_BASEç¯å¢ƒå˜é‡"
            )
        
        # åˆ›å»ºä¸»æ§åˆ¶å™¨å¹¶è¿è¡Œè¯„ä¼°
        controller = MainController(config)
        results = await controller.run_evaluation()
        
        if "error" in results:
            return EvaluationResponse(
                success=False,
                message=f"Ragasè¯„ä¼°å¤±è´¥: {results['error']}"
            )
        
        # æ ¼å¼åŒ–ç»“æœï¼ˆä¸åŒ…å«raw_resultsï¼Œé¿å…åºåˆ—åŒ–é—®é¢˜ï¼‰
        formatted_results = {
            "context_recall": results.get("context_recall", 0),
            "context_precision": results.get("context_precision", 0),
            "faithfulness": results.get("faithfulness", 0),
            "answer_relevancy": results.get("answer_relevancy", 0),
            "context_entity_recall": results.get("context_entity_recall", 0),
            "context_relevance": results.get("context_relevance", 0),
            "answer_correctness": results.get("answer_correctness", 0),
            "answer_similarity": results.get("answer_similarity", 0),
            "fallback_mode": results.get("fallback_mode", False),
            "error_message": results.get("error_message", "")
        }
        
        # ä¿å­˜å®Œæ•´çš„è¯„ä¼°ç»“æœåˆ°å…¨å±€å˜é‡
        global ragas_results
        ragas_results = {
            "context_recall": results.get("context_recall", 0),
            "context_precision": results.get("context_precision", 0),
            "faithfulness": results.get("faithfulness", 0),
            "answer_relevancy": results.get("answer_relevancy", 0),
            "context_entity_recall": results.get("context_entity_recall", 0),
            "context_relevance": results.get("context_relevance", 0),
            "answer_correctness": results.get("answer_correctness", 0),
            "answer_similarity": results.get("answer_similarity", 0),
            "raw_results": results.get("raw_results"),
            "fallback_mode": results.get("fallback_mode", False),
            "error_message": results.get("error_message", ""),
            "evaluation_completed": True,  # æ ‡è®°è¯„ä¼°å·²å®Œæˆ
            "evaluation_time": results.get("evaluation_time", None),
            "dataset_file": dataset_file  # ä¿å­˜ä½¿ç”¨çš„æ•°æ®é›†æ–‡ä»¶
        }
        
        info_print(f"âœ… Ragasè¯„ä¼°ç»“æœå·²ä¿å­˜åˆ°å…¨å±€å˜é‡ï¼Œfallback_mode: {ragas_results.get('fallback_mode', False)}")
        
        return EvaluationResponse(
            success=True,
            message="Ragasè¯„ä¼°å®Œæˆ",
            data=formatted_results
        )
        
    except Exception as e:
        error_msg = f"Ragasè¯„ä¼°å¼‚å¸¸: {str(e)}"
        info_print(error_msg)
        traceback.print_exc()
        return EvaluationResponse(
            success=False,
            message=error_msg
        )

@app.get("/api/bm25/details", response_model=EvaluationResponse)
async def get_bm25_details():
    """è·å–BM25è¯„ä¼°è¯¦æƒ…"""
    global bm25_results
    
    info_print(f"ğŸ” è°ƒè¯•: BM25è¯¦æƒ…APIè°ƒç”¨ï¼Œbm25_resultsçŠ¶æ€: {bm25_results is not None}")
    if bm25_results:
        info_print(f"ğŸ” è°ƒè¯•: bm25_resultsåŒ…å«{len(bm25_results.get('detailed_results', []))}ä¸ªè¯¦ç»†ç»“æœ")
    
    if not bm25_results:
        return EvaluationResponse(
            success=False,
            message="è¯·å…ˆè¿è¡ŒBM25è¯„ä¼°"
        )
    
    # æ ¼å¼åŒ–è¯¦ç»†ç»“æœ
    details = []
    sample_analysis = {}
    
    # ç»„ç»‡ä¸ç›¸å…³åˆ†å—
    for chunk_info in bm25_results.get('irrelevant_chunks', []):
        row_idx = chunk_info['row_index']
        if row_idx not in sample_analysis:
            sample_analysis[row_idx] = {
                'user_input': chunk_info['user_input'],
                'irrelevant_chunks': [],
                'missed_chunks': [],
                'relevant_chunks': []
            }
        sample_analysis[row_idx]['irrelevant_chunks'].append(chunk_info)
    
    # ç»„ç»‡æœªå¬å›åˆ†å—
    for chunk_info in bm25_results.get('missed_chunks', []):
        row_idx = chunk_info['row_index']
        if row_idx not in sample_analysis:
            sample_analysis[row_idx] = {
                'user_input': chunk_info['user_input'],
                'irrelevant_chunks': [],
                'missed_chunks': [],
                'relevant_chunks': []
            }
        sample_analysis[row_idx]['missed_chunks'].append(chunk_info)
    
    # ç»„ç»‡ç›¸å…³åˆ†å—ï¼ˆä¸ä¸ç›¸å…³åˆ†å—ã€æœªå¬å›åˆ†å—ä¿æŒä¸€è‡´ï¼‰
    for chunk_info in bm25_results.get('relevant_chunks', []):
        row_idx = chunk_info['row_index']
        if row_idx not in sample_analysis:
            sample_analysis[row_idx] = {
                'user_input': chunk_info['user_input'],
                'irrelevant_chunks': [],
                'missed_chunks': [],
                'relevant_chunks': []
            }
        sample_analysis[row_idx]['relevant_chunks'].append(chunk_info)
    
    # è½¬æ¢ä¸ºåˆ—è¡¨æ ¼å¼
    for sample_idx, (row_idx, data) in enumerate(sample_analysis.items(), 1):
        details.append({
            'sample_id': sample_idx,
            'row_index': row_idx + 1,
            'user_input': data['user_input'],
            'relevant_chunks': data['relevant_chunks'],
            'irrelevant_chunks': data['irrelevant_chunks'],
            'missed_chunks': data['missed_chunks']
        })
    
    return EvaluationResponse(
        success=True,
        message="è·å–BM25è¯¦æƒ…æˆåŠŸ",
        data={'details': details}
    )

def get_chunk_ragas_scores(ragas_results, sample_id):
    """
    ä»RAGASåŸå§‹ç»“æœä¸­è·å–æŒ‡å®šæ ·æœ¬æ¯ä¸ªåˆ†å—çš„è¯¦ç»†åˆ†æ•°
    
    Args:
        ragas_results: RAGASè¯„ä¼°ç»“æœ
        sample_id: æ ·æœ¬ID (1-based)
        
    Returns:
        dict: åŒ…å«æ¯ä¸ªåˆ†å—è¯„åˆ†çš„å­—å…¸
    """
    try:
        raw_results = ragas_results.get('raw_results')
        if not raw_results:
            return {}
        
        chunk_scores = {}
        
        # ä»tracesä¸­è·å–chunk-levelè¯„åˆ†
        if isinstance(raw_results, dict) and 'traces' in raw_results:
            traces = raw_results['traces']
            
            # sample_idæ˜¯1-basedï¼Œéœ€è¦è½¬æ¢ä¸º0-basedç´¢å¼•
            sample_index = sample_id - 1
            
            if isinstance(traces, list) and 0 <= sample_index < len(traces):
                trace = traces[sample_index]
                
                if isinstance(trace, dict) and 'scores' in trace:
                    scores = trace['scores']
                    
                    # æå–æ‰€æœ‰æŒ‡æ ‡çš„åˆ†æ•°
                    if isinstance(scores, dict):
                        chunk_scores = {
                            'faithfulness': scores.get('faithfulness'),
                            'answer_relevancy': scores.get('answer_relevancy'),
                            'context_precision': scores.get('context_precision'),
                            'context_recall': scores.get('context_recall'),
                            'context_entity_recall': scores.get('context_entity_recall'),
                            'context_relevance': scores.get('nv_context_relevance'),
                            'answer_correctness': scores.get('answer_correctness'),
                            'answer_similarity': scores.get('answer_similarity')
                        }
        
        return chunk_scores
        
    except Exception as e:
        info_print(f"è·å–æ ·æœ¬{sample_id}çš„åˆ†å—RAGASåˆ†æ•°æ—¶å‡ºé”™: {e}")
        return {}

def get_sample_ragas_scores(ragas_results, sample_id):
    """
    ä»RAGASåŸå§‹ç»“æœä¸­è·å–æŒ‡å®šæ ·æœ¬çš„è¯¦ç»†åˆ†æ•°
    
    Args:
        ragas_results: RAGASè¯„ä¼°ç»“æœ
        sample_id: æ ·æœ¬ID (1-based)
        
    Returns:
        tuple: (precision, recall) æˆ– (None, None) å¦‚æœæ— æ³•è·å–
    """
    try:
        raw_results = ragas_results.get('raw_results')
        if not raw_results:
            return None, None
        
        # å¤„ç†ä¸åŒçš„raw_resultsæ ¼å¼
        precision = None
        recall = None
        
        # æ–¹å¼1: å¦‚æœæ˜¯EvaluationResultå¯¹è±¡ï¼Œä½¿ç”¨_scores_dictå±æ€§
        if hasattr(raw_results, '_scores_dict') and raw_results._scores_dict:
            # sample_idæ˜¯1-basedï¼Œéœ€è¦è½¬æ¢ä¸º0-basedç´¢å¼•
            sample_index = sample_id - 1
            
            # è·å–context_precisionå’Œcontext_recallçš„æ‰€æœ‰æ ·æœ¬åˆ†æ•°
            precision_scores = raw_results._scores_dict.get('context_precision', [])
            recall_scores = raw_results._scores_dict.get('context_recall', [])
            
            if (0 <= sample_index < len(precision_scores) and 
                0 <= sample_index < len(recall_scores)):
                
                precision = precision_scores[sample_index]
                recall = recall_scores[sample_index]
        
        # æ–¹å¼2: å¦‚æœæ˜¯å­—å…¸æ ¼å¼ï¼Œä»scoreså­—æ®µä¸­è·å–
        elif isinstance(raw_results, dict) and 'scores' in raw_results:
            scores = raw_results['scores']
            
            # å¦‚æœscoresæ˜¯DataFrame
            if hasattr(scores, 'iloc'):
                # sample_idæ˜¯1-basedï¼Œéœ€è¦è½¬æ¢ä¸º0-basedç´¢å¼•
                sample_index = sample_id - 1
                
                if (0 <= sample_index < len(scores) and 
                    'context_precision' in scores.columns and 
                    'context_recall' in scores.columns):
                    
                    precision = scores.iloc[sample_index]['context_precision']
                    recall = scores.iloc[sample_index]['context_recall']
            
            # å¦‚æœscoresæ˜¯åˆ—è¡¨æ ¼å¼ï¼ˆæ¯ä¸ªå…ƒç´ æ˜¯ä¸€ä¸ªå­—å…¸ï¼‰
            elif isinstance(scores, list):
                # sample_idæ˜¯1-basedï¼Œéœ€è¦è½¬æ¢ä¸º0-basedç´¢å¼•
                sample_index = sample_id - 1
                
                if (0 <= sample_index < len(scores) and 
                    isinstance(scores[sample_index], dict)):
                    
                    sample_scores = scores[sample_index]
                    precision = sample_scores.get('context_precision')
                    recall = sample_scores.get('context_recall')
        
        # æ£€æŸ¥åˆ†æ•°æ˜¯å¦æœ‰æ•ˆï¼ˆä¸æ˜¯NaNæˆ–Noneï¼‰
        if (precision is not None and not (isinstance(precision, float) and str(precision) == 'nan') and
            recall is not None and not (isinstance(recall, float) and str(recall) == 'nan')):
            return float(precision), float(recall)
        
        return None, None
        
    except Exception as e:
        info_print(f"è·å–æ ·æœ¬{sample_id}çš„RAGASåˆ†æ•°æ—¶å‡ºé”™: {e}")
        return None, None

def generate_sample_summary(details, ragas_results):
    """ç”Ÿæˆæ ·æœ¬æ±‡æ€»åˆ†æ"""
    summary = {
        'overall_metrics': {},
        'sample_analysis': []
    }
    
    # è·å–æ•´ä½“è¯„ä¼°æŒ‡æ ‡
    if ragas_results and ragas_results.get('evaluation_completed'):
        summary['overall_metrics'] = {
            'context_precision': ragas_results.get('context_precision', 0),
            'context_recall': ragas_results.get('context_recall', 0),
            'faithfulness': ragas_results.get('faithfulness', 0),
            'answer_relevancy': ragas_results.get('answer_relevancy', 0)
        }
    
    # åˆ†ææ¯ä¸ªæ ·æœ¬
    for detail in details:
        sample_id = detail['sample_id']
        user_input = detail['user_input']
        relevant_chunks = detail['relevant_chunks']
        irrelevant_chunks = detail['irrelevant_chunks']
        missed_chunks = detail['missed_chunks']
        
        # è®¡ç®—æ ·æœ¬ç»Ÿè®¡
        total_retrieved = len(relevant_chunks) + len(irrelevant_chunks)
        total_reference = len(relevant_chunks) + len(missed_chunks)
        
        # å°è¯•ä»RAGASåŸå§‹ç»“æœä¸­è·å–æ¯ä¸ªæ ·æœ¬çš„çœŸå®åˆ†æ•°
        sample_precision, sample_recall = get_sample_ragas_scores(ragas_results, sample_id)
        
        # è·å–è¯¥æ ·æœ¬çš„æ‰€æœ‰RAGASè¯„åˆ†
        chunk_ragas_scores = get_chunk_ragas_scores(ragas_results, sample_id)
        
        # å¦‚æœæ— æ³•è·å–RAGASåˆ†æ•°ï¼Œå›é€€åˆ°åˆ†å—åŒ¹é…è®¡ç®—
        if sample_precision is None or sample_recall is None:
            sample_precision = len(relevant_chunks) / total_retrieved if total_retrieved > 0 else 0
            sample_recall = len(relevant_chunks) / total_reference if total_reference > 0 else 0
        
        # ä¸ºæ¯ä¸ªåˆ†å—æ·»åŠ RAGASè¯„åˆ†
        enhanced_relevant_chunks = []
        for chunk in relevant_chunks:
            enhanced_chunk = chunk.copy()
            enhanced_chunk['ragas_scores'] = chunk_ragas_scores
            enhanced_relevant_chunks.append(enhanced_chunk)
        
        enhanced_irrelevant_chunks = []
        for chunk in irrelevant_chunks:
            enhanced_chunk = chunk.copy()
            enhanced_chunk['ragas_scores'] = chunk_ragas_scores
            enhanced_irrelevant_chunks.append(enhanced_chunk)
        
        enhanced_missed_chunks = []
        for chunk in missed_chunks:
            enhanced_chunk = chunk.copy()
            enhanced_chunk['ragas_scores'] = chunk_ragas_scores
            enhanced_missed_chunks.append(enhanced_chunk)
        
        # ç”Ÿæˆæ ·æœ¬åˆ†ææè¿°
        analysis_desc = generate_sample_description(
            user_input, sample_precision, sample_recall, 
            len(relevant_chunks), len(irrelevant_chunks), len(missed_chunks), sample_id
        )
        
        summary['sample_analysis'].append({
            'sample_id': sample_id,
            'user_input': user_input,
            'precision': sample_precision,
            'recall': sample_recall,
            'relevant_chunks': len(relevant_chunks),
            'irrelevant_chunks': len(irrelevant_chunks),
            'missed_chunks': len(missed_chunks),
            'analysis': analysis_desc,
            'ragas_scores': chunk_ragas_scores,
            'enhanced_relevant_chunks': enhanced_relevant_chunks,
            'enhanced_irrelevant_chunks': enhanced_irrelevant_chunks,
            'enhanced_missed_chunks': enhanced_missed_chunks
        })
    
    return summary

def generate_sample_description(user_input, precision, recall, relevant_count, irrelevant_count, missed_count, sample_id=None):
    """ç”Ÿæˆæ ·æœ¬åˆ†ææè¿°"""
    # æˆªå–æŸ¥è¯¢çš„å‰30ä¸ªå­—ç¬¦
    query_short = user_input[:30] + "..." if len(user_input) > 30 else user_input
    
    # ä½¿ç”¨æ ·æœ¬IDæˆ–é»˜è®¤æè¿°
    sample_desc = f"æ ·æœ¬{sample_id}" if sample_id else "è¯¥æ ·æœ¬"
    
    if precision >= 0.9 and recall >= 0.9:
        return f"{sample_desc}: æ£€ç´¢å†…å®¹å®Œå…¨ç›¸å…³ä¸”å®Œæ•´ï¼ŒPrecision: {precision*100:.1f}%, Recall: {recall*100:.1f}%"
    elif precision >= 0.7 and recall >= 0.7:
        return f"{sample_desc}: æ£€ç´¢è´¨é‡è‰¯å¥½ï¼Œä½†å­˜åœ¨å°‘é‡ä¸ç›¸å…³å†…å®¹ï¼ŒPrecision: {precision*100:.1f}%, Recall: {recall*100:.1f}%"
    elif irrelevant_count > 0 and missed_count > 0:
        return f"{sample_desc}: æ£€ç´¢å†…å®¹ä¸å®Œæ•´ä¸”åŒ…å«ä¸ç›¸å…³ä¿¡æ¯ï¼Œç¼ºå°‘{missed_count}ä¸ªç›¸å…³åˆ†å—ï¼ŒPrecision: {precision*100:.1f}%, Recall: {recall*100:.1f}%"
    elif irrelevant_count > 0:
        return f"{sample_desc}: æ£€ç´¢åˆ°{irrelevant_count}ä¸ªä¸ç›¸å…³åˆ†å—ï¼Œä½†ç›¸å…³åˆ†å—å®Œæ•´ï¼ŒPrecision: {precision*100:.1f}%, Recall: {recall*100:.1f}%"
    elif missed_count > 0:
        return f"{sample_desc}: æ£€ç´¢å†…å®¹ä¸å®Œæ•´ï¼Œç¼ºå°‘{missed_count}ä¸ªç›¸å…³åˆ†å—ï¼ŒPrecision: {precision*100:.1f}%, Recall: {recall*100:.1f}%"
    else:
        return f"{sample_desc}: æ£€ç´¢è´¨é‡ä¸­ç­‰ï¼ŒPrecision: {precision*100:.1f}%, Recall: {recall*100:.1f}%"

@app.get("/api/ragas/details", response_model=EvaluationResponse)
async def get_ragas_details():
    """è·å–Ragasè¯„ä¼°è¯¦æƒ…"""
    global ragas_results
    
    # æ£€æŸ¥æ˜¯å¦æœ‰è¯„ä¼°ç»“æœ
    if not ragas_results or not ragas_results.get('evaluation_completed', False):
        return EvaluationResponse(
            success=False,
            message="è¯·å…ˆè¿è¡ŒRagasè¯„ä¼°"
        )
    
    # æ£€æŸ¥æ˜¯å¦æ˜¯fallbackæ¨¡å¼
    is_fallback = ragas_results.get('fallback_mode', False)
    if is_fallback:
        return EvaluationResponse(
            success=False,
            message=f"Ragasè¯„ä¼°å¤„äºfallbackæ¨¡å¼ï¼Œæ— æ³•æä¾›è¯¦ç»†åˆ†æã€‚é”™è¯¯ä¿¡æ¯: {ragas_results.get('error_message', 'æœªçŸ¥é”™è¯¯')}"
        )
    
    try:
        # è·å–è¯„ä¼°æ—¶ä½¿ç”¨çš„æ•°æ®é›†æ–‡ä»¶
        dataset_file = ragas_results.get('dataset_file', 'standardDataset.xlsx')
        excel_file_path = f"standardDataset/{dataset_file}"
        
        info_print(f"ğŸ“Š æŸ¥çœ‹Ragasæ˜ç»†ï¼Œä½¿ç”¨æ•°æ®é›†: {dataset_file}")
        
        # ä»åŸå§‹æ•°æ®ä¸­é‡æ–°åŠ è½½å¹¶åˆ†æ
        config = EvaluationConfig(
            api_key=os.getenv("QWEN_API_KEY"),
            api_base=os.getenv("QWEN_API_BASE"),
            model_name=os.getenv("QWEN_MODEL_NAME", "qwen-plus"),
            embedding_model=os.getenv("QWEN_EMBEDDING_MODEL", "text-embedding-v1"),
            excel_file_path=excel_file_path  # ä½¿ç”¨è¯„ä¼°æ—¶çš„æ•°æ®é›†
        )
        
        # é‡æ–°åŠ è½½æ•°æ®è¿›è¡Œåˆ†æ
        from read_chuck import DataLoader, TextProcessor
        data_loader = DataLoader(config)
        text_processor = TextProcessor(config)
        
        df = data_loader.load_excel_data()
        if df is None:
            return EvaluationResponse(
                success=False,
                message="æ— æ³•åŠ è½½æ•°æ®æ–‡ä»¶"
            )
        
        # å¤„ç†æ•°æ®
        df = text_processor.parse_context_columns(df)
        
        # åˆ†ææ¯ä¸ªæ ·æœ¬çš„åˆ†å—æƒ…å†µ
        details = []
        for idx, row in df.iterrows():
            user_input = str(row['user_input']) if pd.notna(row['user_input']) else ""
            retrieved_contexts = row['retrieved_contexts']
            reference_contexts = row['reference_contexts']
            
            if not retrieved_contexts or not reference_contexts:
                continue
            
            # ä½¿ç”¨Ragasè¯„ä¼°ç»“æœè¿›è¡Œç›¸å…³æ€§åˆ†æ
            relevant_chunks = []
            irrelevant_chunks = []
            missed_chunks = []
            
            # ä»ç¯å¢ƒå˜é‡è¯»å–ç›¸ä¼¼åº¦é˜ˆå€¼
            similarity_threshold = float(os.getenv("SIMILARITY_THRESHOLD", "0.5"))
            
            # åˆ†æç›¸å…³å’Œä¸ç›¸å…³åˆ†å—
            for i, retrieved_chunk in enumerate(retrieved_contexts):
                # ä½¿ç”¨æ›´æ™ºèƒ½çš„æ–‡æœ¬ç›¸ä¼¼åº¦è®¡ç®—
                is_relevant = False
                max_similarity = 0
                best_ref_idx = -1
                
                for j, ref_chunk in enumerate(reference_contexts):
                    # è®¡ç®—æ–‡æœ¬ç›¸ä¼¼åº¦
                    similarity = calculate_text_similarity(retrieved_chunk, ref_chunk)
                    if similarity > max_similarity:
                        max_similarity = similarity
                        best_ref_idx = j
                
                # å¦‚æœç›¸ä¼¼åº¦è¶³å¤Ÿé«˜ï¼Œè®¤ä¸ºç›¸å…³
                if max_similarity > similarity_threshold:
                    is_relevant = True
                    relevant_chunks.append({
                        'retrieved_chunk': retrieved_chunk,
                        'reference_chunk': reference_contexts[best_ref_idx] if best_ref_idx >= 0 else "",
                        'retrieved_idx': i,
                        'reference_idx': best_ref_idx,
                        'relevance_score': max_similarity
                    })
                else:
                    irrelevant_chunks.append({
                        'retrieved_chunk': retrieved_chunk,
                        'retrieved_idx': i,
                        'max_relevance': max_similarity
                    })
            
            # åˆ†ææœªå¬å›åˆ†å—
            # æœªå¬å›åˆ†å— = reference_contextsä¸­å­˜åœ¨çš„åˆ†å—ï¼Œè€Œretrieved_contextsä¸­ä¸å­˜åœ¨çš„åˆ†å—
            matched_references = set()
            for chunk in relevant_chunks:
                if 'reference_idx' in chunk:
                    matched_references.add(chunk['reference_idx'])
            
            for j, ref_chunk in enumerate(reference_contexts):
                if j not in matched_references:
                    # æ‰¾åˆ°è¯¥å‚è€ƒåˆ†å—ä¸æ‰€æœ‰æ£€ç´¢åˆ†å—çš„æœ€å¤§ç›¸ä¼¼åº¦
                    max_similarity = 0
                    best_retrieved_idx = -1
                    for i, retrieved_chunk in enumerate(retrieved_contexts):
                        similarity = calculate_text_similarity(retrieved_chunk, ref_chunk)
                        if similarity > max_similarity:
                            max_similarity = similarity
                            best_retrieved_idx = i
                    
                    # å¦‚æœç›¸ä¼¼åº¦ä½äºé˜ˆå€¼ï¼Œè®¤ä¸ºæ˜¯æœªå¬å›çš„åˆ†å—
                    if max_similarity < similarity_threshold:
                        missed_chunks.append({
                            'reference_chunk': ref_chunk,
                            'reference_idx': j,
                            'best_retrieved_idx': best_retrieved_idx,
                            'max_relevance': max_similarity
                        })
            
            details.append({
                'sample_id': len(details) + 1,
                'row_index': idx + 1,
                'user_input': user_input,
                'relevant_chunks': relevant_chunks,
                'irrelevant_chunks': irrelevant_chunks,
                'missed_chunks': missed_chunks
            })
        
        # ç”Ÿæˆæ ·æœ¬æ±‡æ€»åˆ†æ
        sample_summary = generate_sample_summary(details, ragas_results)
        
        return EvaluationResponse(
            success=True,
            message="è·å–Ragasè¯¦æƒ…æˆåŠŸ",
            data={
                'details': details,
                'sample_summary': sample_summary,
                'ragas_raw_results': ragas_results.get('raw_results', None)
            }
        )
        
    except Exception as e:
        import traceback
        traceback.print_exc()
        return EvaluationResponse(
            success=False,
            message=f"è·å–Ragasè¯¦æƒ…å¤±è´¥: {str(e)}"
        )

@app.post("/api/save-bm25-combined", response_model=SaveEvaluationResponse)
async def save_bm25_combined(request: BM25CombinedResults):
    """ä¿å­˜åŒ…å«æ‰€æœ‰æŒ‡æ ‡çš„BM25è¯„ä¼°ç»“æœ"""
    try:
        # æ„å»ºBM25ç»“æœæ•°æ®
        bm25_data = {
            'avg_precision': request.context_precision,
            'avg_recall': request.context_recall,
            'avg_f1': request.f1_score,
            'mrr': request.mrr,
            'map': request.map,
            'ndcg': request.ndcg,
            'total_samples': request.total_samples,
            'irrelevant_chunks': [{}] * request.irrelevant_chunks,  # åˆ›å»ºç©ºæ•°ç»„
            'missed_chunks': [{}] * request.missed_chunks,  # åˆ›å»ºç©ºæ•°ç»„
            'relevant_chunks': [{}] * request.relevant_chunks  # åˆ›å»ºç©ºæ•°ç»„
        }
        
        # ä¿å­˜åˆ°æ•°æ®åº“
        evaluation_id = db.save_bm25_result(bm25_data, request.description)
        
        if evaluation_id:
            return SaveEvaluationResponse(
                success=True,
                message="BM25è¯„ä¼°ç»“æœä¿å­˜æˆåŠŸ",
                evaluation_id=evaluation_id
            )
        else:
            return SaveEvaluationResponse(
                success=False,
                message="ä¿å­˜BM25è¯„ä¼°ç»“æœå¤±è´¥"
            )
            
    except Exception as e:
        import traceback
        traceback.print_exc()
        return SaveEvaluationResponse(
            success=False,
            message=f"ä¿å­˜BM25è¯„ä¼°ç»“æœå¤±è´¥: {str(e)}"
        )

@app.post("/api/save-evaluation", response_model=SaveEvaluationResponse)
async def save_evaluation(request: SaveEvaluationRequest):
    """ä¿å­˜è¯„ä¼°ç»“æœåˆ°æ•°æ®åº“"""
    try:
        # æ£€æŸ¥è¯„ä¼°ç±»å‹
        if request.evaluation_type not in ["BM25", "RAGAS"]:
            return SaveEvaluationResponse(
                success=False,
                message="æ— æ•ˆçš„è¯„ä¼°ç±»å‹ï¼Œå¿…é¡»æ˜¯BM25æˆ–RAGAS"
            )
        
        # è·å–å¯¹åº”çš„è¯„ä¼°ç»“æœ
        if request.evaluation_type == "BM25":
            if not bm25_results:
                return SaveEvaluationResponse(
                    success=False,
                    message="æ²¡æœ‰BM25è¯„ä¼°ç»“æœå¯ä¿å­˜ï¼Œè¯·å…ˆè¿è¡Œè¯„ä¼°"
                )
            # ä½¿ç”¨åŸå§‹çš„BM25ç»“æœï¼Œä½†éœ€è¦ç¡®ä¿åŒ…å«æ‰€æœ‰æŒ‡æ ‡
            results = bm25_results.copy()
            
            # å¦‚æœç»“æœä¸­æ²¡æœ‰æ–°æŒ‡æ ‡ï¼Œå°è¯•ä»å…¨å±€å˜é‡è·å–
            if 'mrr' not in results or 'map' not in results or 'ndcg' not in results:
                info_print("âš ï¸ BM25ç»“æœä¸­ç¼ºå°‘MRR/MAP/NDCGæŒ‡æ ‡ï¼Œä½¿ç”¨é»˜è®¤å€¼0")
                results.setdefault('mrr', 0)
                results.setdefault('map', 0)
                results.setdefault('ndcg', 0)
        else:  # RAGAS
            if not ragas_results:
                return SaveEvaluationResponse(
                    success=False,
                    message="æ²¡æœ‰Ragasè¯„ä¼°ç»“æœå¯ä¿å­˜ï¼Œè¯·å…ˆè¿è¡Œè¯„ä¼°"
                )
            results = ragas_results
        
        # ä¿å­˜åˆ°æ•°æ®åº“
        if request.evaluation_type == "BM25":
            evaluation_id = db.save_bm25_result(results, request.description)
        else:
            # å¯¹äºRagasè¯„ä¼°ï¼Œå…ˆæå–ç»Ÿè®¡æ•°æ®ï¼Œç„¶åä¿å­˜
            info_print("ğŸ“Š æå–Ragasè¯„ä¼°ç»Ÿè®¡æ•°æ®...")
            stats = db.extract_ragas_statistics(results)
            info_print(f"âœ… ç»Ÿè®¡æ•°æ®æå–å®Œæˆ: {stats}")
            evaluation_id = db.save_ragas_result(results, request.description)
        
        if evaluation_id:
            return SaveEvaluationResponse(
                success=True,
                message=f"{request.evaluation_type}è¯„ä¼°ç»“æœä¿å­˜æˆåŠŸ",
                evaluation_id=evaluation_id
            )
        else:
            return SaveEvaluationResponse(
                success=False,
                message="ä¿å­˜è¯„ä¼°ç»“æœå¤±è´¥"
            )
            
    except Exception as e:
        import traceback
        traceback.print_exc()
        return SaveEvaluationResponse(
            success=False,
            message=f"ä¿å­˜è¯„ä¼°ç»“æœå¤±è´¥: {str(e)}"
        )

@app.get("/api/evaluation-history")
async def get_evaluation_history():
    """è·å–è¯„ä¼°å†å²è®°å½•"""
    try:
        history = db.get_evaluation_history()
        return EvaluationResponse(
            success=True,
            message="è·å–è¯„ä¼°å†å²æˆåŠŸ",
            data={'history': history}
        )
    except Exception as e:
        return EvaluationResponse(
            success=False,
            message=f"è·å–è¯„ä¼°å†å²å¤±è´¥: {str(e)}"
        )

@app.get("/api/ragas-status")
async def get_ragas_status():
    """è·å–Ragasè¯„ä¼°çŠ¶æ€"""
    global ragas_results
    
    try:
        if not ragas_results:
            return EvaluationResponse(
                success=False,
                message="æ²¡æœ‰Ragasè¯„ä¼°ç»“æœ",
                data={'has_results': False}
            )
        
        return EvaluationResponse(
            success=True,
            message="Ragasè¯„ä¼°çŠ¶æ€æ­£å¸¸",
            data={
                'has_results': True,
                'evaluation_completed': ragas_results.get('evaluation_completed', False),
                'fallback_mode': ragas_results.get('fallback_mode', False),
                'error_message': ragas_results.get('error_message', ''),
                'has_metrics': any(ragas_results.get(metric) is not None for metric in 
                                 ['faithfulness', 'answer_relevancy', 'context_precision', 'context_recall'])
            }
        )
    except Exception as e:
        return EvaluationResponse(
            success=False,
            message=f"æ£€æŸ¥RagasçŠ¶æ€å¤±è´¥: {str(e)}",
            data={'has_results': False}
        )

@app.get("/api/database-status")
async def get_database_status():
    """è·å–æ•°æ®åº“è¿æ¥çŠ¶æ€"""
    try:
        is_connected = test_connection()
        if is_connected:
            stats = db.get_database_statistics()
            return EvaluationResponse(
                success=True,
                message="æ•°æ®åº“è¿æ¥æ­£å¸¸",
                data={'connected': True, 'statistics': stats}
            )
        else:
            return EvaluationResponse(
                success=False,
                message="æ•°æ®åº“è¿æ¥å¤±è´¥",
                data={'connected': False}
            )
    except Exception as e:
        return EvaluationResponse(
            success=False,
            message=f"æ£€æŸ¥æ•°æ®åº“çŠ¶æ€å¤±è´¥: {str(e)}",
            data={'connected': False}
        )

@app.post("/api/init-database")
async def init_database_endpoint():
    """åˆå§‹åŒ–æ•°æ®åº“è¡¨"""
    try:
        success = init_database()
        if success:
            return EvaluationResponse(
                success=True,
                message="æ•°æ®åº“è¡¨åˆå§‹åŒ–æˆåŠŸ"
            )
        else:
            return EvaluationResponse(
                success=False,
                message="æ•°æ®åº“è¡¨åˆå§‹åŒ–å¤±è´¥"
            )
    except Exception as e:
        return EvaluationResponse(
            success=False,
            message=f"æ•°æ®åº“è¡¨åˆå§‹åŒ–å¤±è´¥: {str(e)}"
        )

# å†å²æ•°æ®åˆ†æAPIæ¥å£
@app.get("/api/history/bm25/precision")
@cache_response(cache_instance=get_history_cache())
async def get_bm25_precision_history():
    """è·å–BM25å‡†ç¡®ç‡å†å²æ•°æ®ï¼ˆå¸¦ç¼“å­˜ï¼‰"""
    try:
        from database.db_service import get_evaluation_history
        
        # è·å–BM25è¯„ä¼°å†å²æ•°æ®
        data = get_evaluation_history('BM25', 'context_precision')
        
        return EvaluationResponse(
            success=True,
            message="è·å–BM25å‡†ç¡®ç‡å†å²æ•°æ®æˆåŠŸ",
            data={"history": data}
        )
    except Exception as e:
        return EvaluationResponse(
            success=False,
            message=f"è·å–BM25å‡†ç¡®ç‡å†å²æ•°æ®å¤±è´¥: {str(e)}"
        )

@app.get("/api/history/bm25/recall")
@cache_response(cache_instance=get_history_cache())
async def get_bm25_recall_history():
    """è·å–BM25å¬å›ç‡å†å²æ•°æ®ï¼ˆå¸¦ç¼“å­˜ï¼‰"""
    try:
        from database.db_service import get_evaluation_history
        
        # è·å–BM25è¯„ä¼°å†å²æ•°æ®
        data = get_evaluation_history('BM25', 'context_recall')
        
        return EvaluationResponse(
            success=True,
            message="è·å–BM25å¬å›ç‡å†å²æ•°æ®æˆåŠŸ",
            data={"history": data}
        )
    except Exception as e:
        return EvaluationResponse(
            success=False,
            message=f"è·å–BM25å¬å›ç‡å†å²æ•°æ®å¤±è´¥: {str(e)}"
        )

@app.get("/api/history/bm25/f1")
@cache_response(cache_instance=get_history_cache())
async def get_bm25_f1_history():
    """è·å–BM25 F1-Scoreå†å²æ•°æ®ï¼ˆå¸¦ç¼“å­˜ï¼‰"""
    try:
        from database.db_service import get_evaluation_history
        
        # è·å–BM25è¯„ä¼°å†å²æ•°æ®
        data = get_evaluation_history('BM25', 'f1_score')
        
        return EvaluationResponse(
            success=True,
            message="è·å–BM25 F1-Scoreå†å²æ•°æ®æˆåŠŸ",
            data={"history": data}
        )
    except Exception as e:
        return EvaluationResponse(
            success=False,
            message=f"è·å–BM25 F1-Scoreå†å²æ•°æ®å¤±è´¥: {str(e)}"
        )

@app.get("/api/history/bm25/ndcg")
@cache_response(cache_instance=get_history_cache())
async def get_bm25_ndcg_history():
    """è·å–BM25 NDCGå†å²æ•°æ®ï¼ˆå¸¦ç¼“å­˜ï¼‰"""
    try:
        from database.db_service import get_evaluation_history
        
        # è·å–BM25è¯„ä¼°å†å²æ•°æ®
        data = get_evaluation_history('BM25', 'ndcg')
        
        return EvaluationResponse(
            success=True,
            message="è·å–BM25 NDCGå†å²æ•°æ®æˆåŠŸ",
            data={"history": data}
        )
    except Exception as e:
        return EvaluationResponse(
            success=False,
            message=f"è·å–BM25 NDCGå†å²æ•°æ®å¤±è´¥: {str(e)}"
        )

@app.get("/api/history/ragas/precision")
@cache_response(cache_instance=get_history_cache())
async def get_ragas_precision_history():
    """è·å–Ragaså‡†ç¡®ç‡å†å²æ•°æ®ï¼ˆå¸¦ç¼“å­˜ï¼‰"""
    try:
        from database.db_service import get_evaluation_history
        
        # è·å–Ragasè¯„ä¼°å†å²æ•°æ®
        data = get_evaluation_history('RAGAS', 'context_precision')
        
        return EvaluationResponse(
            success=True,
            message="è·å–Ragaså‡†ç¡®ç‡å†å²æ•°æ®æˆåŠŸ",
            data={"history": data}
        )
    except Exception as e:
        return EvaluationResponse(
            success=False,
            message=f"è·å–Ragaså‡†ç¡®ç‡å†å²æ•°æ®å¤±è´¥: {str(e)}"
        )

@app.get("/api/history/ragas/recall")
@cache_response(cache_instance=get_history_cache())
async def get_ragas_recall_history():
    """è·å–Ragaså¬å›ç‡å†å²æ•°æ®ï¼ˆå¸¦ç¼“å­˜ï¼‰"""
    try:
        from database.db_service import get_evaluation_history
        
        # è·å–Ragasè¯„ä¼°å†å²æ•°æ®
        data = get_evaluation_history('RAGAS', 'context_recall')
        
        return EvaluationResponse(
            success=True,
            message="è·å–Ragaså¬å›ç‡å†å²æ•°æ®æˆåŠŸ",
            data={"history": data}
        )
    except Exception as e:
        return EvaluationResponse(
            success=False,
            message=f"è·å–Ragaså¬å›ç‡å†å²æ•°æ®å¤±è´¥: {str(e)}"
        )

@app.get("/api/history/stats")
@cache_response(cache_instance=get_stats_cache())
async def get_history_stats():
    """è·å–å†å²æ•°æ®ç»Ÿè®¡æ¦‚è§ˆï¼ˆå¸¦ç¼“å­˜ï¼‰"""
    try:
        from database.db_service import get_evaluation_stats
        
        # è·å–ç»Ÿè®¡æ¦‚è§ˆæ•°æ®
        stats = get_evaluation_stats()
        
        return EvaluationResponse(
            success=True,
            message="è·å–å†å²æ•°æ®ç»Ÿè®¡æˆåŠŸ",
            data=stats
        )
    except Exception as e:
        return EvaluationResponse(
            success=False,
            message=f"è·å–å†å²æ•°æ®ç»Ÿè®¡å¤±è´¥: {str(e)}"
        )

@app.get("/api/cache/stats")
async def get_cache_stats():
    """è·å–ç¼“å­˜ç»Ÿè®¡ä¿¡æ¯"""
    try:
        stats = get_all_cache_stats()
        return EvaluationResponse(
            success=True,
            message="è·å–ç¼“å­˜ç»Ÿè®¡æˆåŠŸ",
            data=stats
        )
    except Exception as e:
        return EvaluationResponse(
            success=False,
            message=f"è·å–ç¼“å­˜ç»Ÿè®¡å¤±è´¥: {str(e)}"
        )

@app.post("/api/cache/clear")
async def clear_cache():
    """æ¸…ç©ºæ‰€æœ‰ç¼“å­˜"""
    try:
        cleared_counts = clear_all_caches()
        return EvaluationResponse(
            success=True,
            message=f"ç¼“å­˜å·²æ¸…ç©ºï¼Œå…±æ¸…é™¤ {cleared_counts['total']} é¡¹",
            data=cleared_counts
        )
    except Exception as e:
        return EvaluationResponse(
            success=False,
            message=f"æ¸…ç©ºç¼“å­˜å¤±è´¥: {str(e)}"
        )

@app.get("/api/history/all")
@cache_response(cache_instance=get_history_cache())
async def get_all_history_data():
    """
    æ‰¹é‡è·å–æ‰€æœ‰å†å²æ•°æ®ï¼ˆä¼˜åŒ–ç‰ˆï¼‰
    ä¸€æ¬¡æ€§è¿”å›æ‰€æœ‰å›¾è¡¨éœ€è¦çš„æ•°æ®ï¼Œå‡å°‘APIè°ƒç”¨æ¬¡æ•°
    """
    try:
        from database.db_service import get_evaluation_history
        
        # æ‰¹é‡æŸ¥è¯¢æ‰€æœ‰æ•°æ®
        result = {
            "bm25": {
                "precision": get_evaluation_history('BM25', 'context_precision'),
                "recall": get_evaluation_history('BM25', 'context_recall'),
                "f1": get_evaluation_history('BM25', 'f1_score'),
                "ndcg": get_evaluation_history('BM25', 'ndcg')
            },
            "ragas": {
                "precision": get_evaluation_history('RAGAS', 'context_precision'),
                "recall": get_evaluation_history('RAGAS', 'context_recall')
            }
        }
        
        return EvaluationResponse(
            success=True,
            message="æ‰¹é‡è·å–å†å²æ•°æ®æˆåŠŸ",
            data=result
        )
    except Exception as e:
        return EvaluationResponse(
            success=False,
            message=f"æ‰¹é‡è·å–å†å²æ•°æ®å¤±è´¥: {str(e)}"
        )

@app.get("/api/embedding-config")
async def get_embedding_config():
    """è·å–å½“å‰embeddingæ¨¡å‹é…ç½®"""
    try:
        # ä¼˜å…ˆä».envæ–‡ä»¶è¯»å–ï¼Œç„¶åä»ç¯å¢ƒå˜é‡è¯»å–
        config = {
            "use_ollama": get_env_value("USE_OLLAMA", os.getenv("USE_OLLAMA", "false")).lower() == "true",
            "ollama_url": get_env_value("OLLAMA_URL", os.getenv("OLLAMA_URL", "http://localhost:11434")),
            "ollama_model": get_env_value("OLLAMA_EMBEDDING_MODEL", os.getenv("OLLAMA_EMBEDDING_MODEL", "embeddinggemma:300m")),
            "ollama_llm_model": get_env_value("OLLAMA_LLM_MODEL", os.getenv("OLLAMA_LLM_MODEL", "qwen2.5:7b")),
            "qwen_embedding_model": get_env_value("QWEN_EMBEDDING_MODEL", os.getenv("QWEN_EMBEDDING_MODEL", "text-embedding-v1")),
            "qwen_api_key": get_env_value("QWEN_API_KEY", os.getenv("QWEN_API_KEY", "")),
            "openai_api_key": get_env_value("OPENAI_API_KEY", os.getenv("OPENAI_API_KEY", ""))
        }
        
        return EvaluationResponse(
            success=True,
            message="è·å–embeddingé…ç½®æˆåŠŸ",
            data=config
        )
    except Exception as e:
        return EvaluationResponse(
            success=False,
            message=f"è·å–embeddingé…ç½®å¤±è´¥: {str(e)}"
        )

class EmbeddingConfigRequest(BaseModel):
    """æ¨¡å‹é…ç½®è¯·æ±‚æ¨¡å‹"""
    use_ollama: bool
    ollama_url: str = "http://localhost:11434"
    ollama_model: str = "embeddinggemma:300m"
    ollama_llm_model: str = "qwen2.5:7b"
    qwen_api_key: str = ""

@app.post("/api/embedding-config")
async def update_embedding_config(request: EmbeddingConfigRequest):
    """æ›´æ–°embeddingæ¨¡å‹é…ç½®"""
    try:
        # æ›´æ–°ç¯å¢ƒå˜é‡ï¼ˆä»…åœ¨å½“å‰ä¼šè¯ä¸­æœ‰æ•ˆï¼‰
        os.environ["USE_OLLAMA"] = str(request.use_ollama).lower()
        os.environ["OLLAMA_URL"] = request.ollama_url
        os.environ["OLLAMA_EMBEDDING_MODEL"] = request.ollama_model
        os.environ["OLLAMA_LLM_MODEL"] = request.ollama_llm_model
        os.environ["QWEN_API_KEY"] = request.qwen_api_key
        
        # å¦‚æœæä¾›äº†Qwen API Keyï¼Œä¹Ÿæ›´æ–°OPENAI_API_KEYï¼ˆç”¨äºå…¼å®¹æ€§ï¼‰
        if request.qwen_api_key:
            os.environ["OPENAI_API_KEY"] = request.qwen_api_key
        
        # åŒæ—¶æ›´æ–°.envæ–‡ä»¶
        env_updates = {
            "USE_OLLAMA": str(request.use_ollama).lower(),
            "OLLAMA_URL": request.ollama_url,
            "OLLAMA_EMBEDDING_MODEL": request.ollama_model,
            "OLLAMA_LLM_MODEL": request.ollama_llm_model,
            "QWEN_API_KEY": request.qwen_api_key
        }
        
        if request.qwen_api_key:
            env_updates["OPENAI_API_KEY"] = request.qwen_api_key
        
        update_env_file(env_updates)
        
        return EvaluationResponse(
            success=True,
            message="Embeddingé…ç½®æ›´æ–°æˆåŠŸ"
        )
    except Exception as e:
        return EvaluationResponse(
            success=False,
            message=f"æ›´æ–°embeddingé…ç½®å¤±è´¥: {str(e)}"
        )

@app.get("/api/dataset-files")
async def get_dataset_files_api():
    """è·å–standardDatasetç›®å½•ä¸‹çš„æ‰€æœ‰æ•°æ®é›†æ–‡ä»¶"""
    try:
        result = get_dataset_files()
        return result
    except Exception as e:
        return {"success": False, "data": [], "message": f"è·å–æ•°æ®é›†æ–‡ä»¶åˆ—è¡¨å¤±è´¥: {str(e)}"}

@app.post("/api/upload-document")
async def upload_document_api(file: UploadFile = File(...)):
    """ä¸Šä¼ å¾…è¯„æµ‹çš„æ–‡æ¡£"""
    try:
        # æ£€æŸ¥æ–‡ä»¶ç±»å‹
        if not file.filename or not file.filename.lower().endswith(('.xlsx', '.xls')):
            return EvaluationResponse(
                success=False,
                message="åªæ”¯æŒExcelæ–‡æ¡£æ ¼å¼(.xlsx, .xls)"
            )
        
        # åˆ›å»ºä¸´æ—¶æ–‡ä»¶
        with tempfile.NamedTemporaryFile(delete=False, suffix='.xlsx') as temp_file:
            # è¯»å–ä¸Šä¼ çš„æ–‡ä»¶å†…å®¹
            content = await file.read()
            temp_file.write(content)
            temp_file_path = temp_file.name
        
        try:
            # è°ƒç”¨ä¸Šä¼ å¤„ç†å‡½æ•°ï¼Œä¼ å…¥åŸå§‹æ–‡ä»¶åç”¨äºç”Ÿæˆæ—¶é—´æˆ³æ–‡ä»¶å
            result = upload_document(temp_file_path, original_filename=file.filename)
            
            if result["success"]:
                return EvaluationResponse(
                    success=True,
                    message=result["message"],
                    data={
                        "file_path": result.get("file_path"),
                        "file_size": result.get("file_size"),
                        "validation": result.get("validation")
                    }
                )
            else:
                return EvaluationResponse(
                    success=False,
                    message=result["message"]
                )
        finally:
            # æ¸…ç†ä¸´æ—¶æ–‡ä»¶
            if os.path.exists(temp_file_path):
                os.unlink(temp_file_path)
                
    except Exception as e:
        import traceback
        error_details = traceback.format_exc()
        info_print(f"ä¸Šä¼ æ–‡æ¡£å¼‚å¸¸: {error_details}")
        return EvaluationResponse(
            success=False,
            message=f"ä¸Šä¼ æ–‡æ¡£å¤±è´¥: {str(e)}"
        )

@app.get("/api/upload-info")
async def get_upload_info_api():
    """è·å–ä¸Šä¼ æ–‡æ¡£ä¿¡æ¯"""
    try:
        info = get_upload_info()
        return EvaluationResponse(
            success=True,
            message="è·å–ä¸Šä¼ ä¿¡æ¯æˆåŠŸ",
            data=info
        )
    except Exception as e:
        return EvaluationResponse(
            success=False,
            message=f"è·å–ä¸Šä¼ ä¿¡æ¯å¤±è´¥: {str(e)}"
        )

@app.delete("/api/uploaded-document")
async def delete_uploaded_document_api():
    """åˆ é™¤å·²ä¸Šä¼ çš„æ–‡æ¡£"""
    try:
        result = delete_uploaded_file()
        return EvaluationResponse(
            success=result["success"],
            message=result["message"]
        )
    except Exception as e:
        return EvaluationResponse(
            success=False,
            message=f"åˆ é™¤æ–‡æ¡£å¤±è´¥: {str(e)}"
        )

@app.get("/api/dataset/download-template")
async def download_template():
    """ä¸‹è½½æ ‡å‡†æ•°æ®é›†æ¨¡ç‰ˆæ–‡ä»¶"""
    try:
        # æ ‡å‡†æ•°æ®é›†æ¨¡ç‰ˆæ–‡ä»¶è·¯å¾„
        template_path = "standardDataset/standardDataset.xlsx"
        
        # æ£€æŸ¥æ–‡ä»¶æ˜¯å¦å­˜åœ¨
        if not os.path.exists(template_path):
            raise HTTPException(status_code=404, detail="æ¨¡ç‰ˆæ–‡ä»¶ä¸å­˜åœ¨")
        
        info_print(f"ğŸ“¥ ä¸‹è½½æ¨¡ç‰ˆæ–‡ä»¶: {template_path}")
        
        # è¿”å›æ–‡ä»¶å“åº”
        return FileResponse(
            path=template_path,
            filename="standardDataset.xlsx",
            media_type="application/vnd.openxmlformats-officedocument.spreadsheetml.sheet",
            headers={
                "Content-Disposition": "attachment; filename=standardDataset.xlsx"
            }
        )
    except Exception as e:
        error_msg = f"ä¸‹è½½æ¨¡ç‰ˆå¤±è´¥: {str(e)}"
        info_print(error_msg)
        raise HTTPException(status_code=500, detail=error_msg)

# çŸ¥è¯†åº“æ–‡æ¡£ä¸Šä¼ ç›¸å…³APIç«¯ç‚¹

@app.post("/api/upload-knowledge-document")
async def upload_knowledge_document_api(file: UploadFile = File(...)):
    """ä¸Šä¼ çŸ¥è¯†åº“æ–‡æ¡£"""
    try:
        # éªŒè¯æ–‡ä»¶ç±»å‹
        allowed_extensions = ['.pdf', '.doc', '.docx', '.txt', '.md']
        file_extension = Path(file.filename).suffix.lower()
        
        if file_extension not in allowed_extensions:
            return {
                "success": False,
                "message": f"ä¸æ”¯æŒçš„æ–‡ä»¶æ ¼å¼: {file_extension}ã€‚æ”¯æŒçš„æ ¼å¼: {', '.join(allowed_extensions)}"
            }
        
        # åˆ›å»ºä¸´æ—¶æ–‡ä»¶
        with tempfile.NamedTemporaryFile(delete=False, suffix=file_extension) as temp_file:
            content = await file.read()
            temp_file.write(content)
            temp_file_path = temp_file.name
        
        try:
            # è°ƒç”¨ä¸Šä¼ å‡½æ•°
            result = upload_knowledge_document(temp_file_path, file.filename)
            return result
        finally:
            # æ¸…ç†ä¸´æ—¶æ–‡ä»¶
            if os.path.exists(temp_file_path):
                os.unlink(temp_file_path)
                
    except Exception as e:
        return {
            "success": False,
            "message": f"ä¸Šä¼ å¤±è´¥: {str(e)}"
        }

@app.get("/api/knowledge-documents")
async def get_knowledge_documents_api():
    """è·å–çŸ¥è¯†åº“æ–‡æ¡£åˆ—è¡¨"""
    try:
        result = get_knowledge_documents()
        return result
    except Exception as e:
        return {"success": False, "message": f"è·å–æ–‡æ¡£åˆ—è¡¨å¤±è´¥: {str(e)}"}

@app.delete("/api/knowledge-documents/{filename}")
async def delete_knowledge_document_api(filename: str):
    """åˆ é™¤æŒ‡å®šçš„çŸ¥è¯†åº“æ–‡æ¡£"""
    try:
        result = delete_knowledge_document(filename)
        return result
    except Exception as e:
        return {"success": False, "message": f"åˆ é™¤æ–‡æ¡£å¤±è´¥: {str(e)}"}

# æ„å»ºæ•°æ®é›†APIç«¯ç‚¹

@app.post("/api/build-dataset")
async def build_dataset_api():
    """æ„å»ºæ ‡å‡†æ•°æ®é›†"""
    try:
        # è°ƒç”¨æ„å»ºæ•°æ®é›†å‡½æ•°
        result = await build_standard_dataset()
        return result
    except Exception as e:
        return {
            "success": False,
            "message": f"æ„å»ºæ•°æ®é›†å¤±è´¥: {str(e)}"
        }

if __name__ == "__main__":
    import uvicorn
    uvicorn.run(app, host="0.0.0.0", port=8006)
