"""
æ•°æ®åº“æ“ä½œæœåŠ¡
å…¼å®¹MySQLå’ŒSQLiteä¸¤ç§æ•°æ®åº“
"""
from typing import List, Optional, Dict, Any
import os
from sqlalchemy.orm import Session
from sqlalchemy import desc, text
from database.db_config import get_db_session
# ç§»é™¤å¯¹EvaluationResultæ¨¡å‹çš„ä¾èµ–ï¼Œå› ä¸ºæˆ‘ä»¬ç°åœ¨ä½¿ç”¨ç‹¬ç«‹çš„è¡¨

class DatabaseService:
    """æ•°æ®åº“æ“ä½œæœåŠ¡ç±»"""
    
    @staticmethod
    def save_bm25_result(results: Dict[str, Any], description: str = "") -> Optional[int]:
        """ä¿å­˜BM25è¯„ä¼°ç»“æœåˆ°ç‹¬ç«‹çš„bm25_evaluationsè¡¨"""
        try:
            with get_db_session() as session:
                # æå–ç»Ÿè®¡æ•°æ®
                total_samples = results.get('total_samples', 0)
                irrelevant_chunks_count = len(results.get('irrelevant_chunks', []))
                missed_chunks_count = len(results.get('missed_chunks', []))
                
                # æå–æŒ‡æ ‡æ•°æ®
                context_precision = results.get('avg_precision', 0)
                context_recall = results.get('avg_recall', 0)
                f1_score = results.get('avg_f1', 0)
                mrr = results.get('mrr', 0)
                map_score = results.get('map', 0)
                ndcg = results.get('ndcg', 0)
                
                # æ„å»ºSQLæŸ¥è¯¢ï¼Œå…¼å®¹MySQLå’ŒSQLite
                sql = """
                INSERT INTO bm25_evaluations 
                (evaluation_time, description, context_precision, context_recall, 
                 f1_score, mrr, map, ndcg, total_samples, irrelevant_chunks_count, missed_chunks_count)
                VALUES (CURRENT_TIMESTAMP, :description, :context_precision, :context_recall, 
                 :f1_score, :mrr, :map_score, :ndcg, :total_samples, :irrelevant_chunks_count, :missed_chunks_count)
                """
                
                result = session.execute(text(sql), {
                    'description': description,
                    'context_precision': context_precision,
                    'context_recall': context_recall,
                    'f1_score': f1_score,
                    'mrr': mrr,
                    'map_score': map_score,
                    'ndcg': ndcg,
                    'total_samples': total_samples,
                    'irrelevant_chunks_count': irrelevant_chunks_count,
                    'missed_chunks_count': missed_chunks_count
                })
                session.commit()
                
                # è·å–æ’å…¥çš„ID
                # å¯¹äºSQLAlchemyï¼Œæˆ‘ä»¬éœ€è¦é‡æ–°æŸ¥è¯¢è·å–ID
                try:
                    # MySQL
                    id_result = session.execute(text("SELECT LAST_INSERT_ID()"))
                except:
                    # SQLite
                    id_result = session.execute(text("SELECT last_insert_rowid()"))
                row = id_result.fetchone()
                return row[0] if row else None
        except Exception as e:
            print(f"ä¿å­˜BM25ç»“æœå¤±è´¥: {e}")
            import traceback
            traceback.print_exc()
            return None
    
    @staticmethod
    def extract_ragas_statistics(results: Dict[str, Any]) -> Dict[str, int]:
        """
        ä»Ragasè¯„ä¼°ç»“æœä¸­æå–ç»Ÿè®¡æ•°æ®
        
        Args:
            results: Ragasè¯„ä¼°ç»“æœ
            
        Returns:
            Dict[str, int]: åŒ…å«total_samples, irrelevant_chunks_count, missed_chunks_countçš„å­—å…¸
        """
        try:
            # å°è¯•ä»raw_resultsä¸­è·å–è¯¦æƒ…æ•°æ®
            raw_results = results.get('raw_results', {})
            total_samples = 0
            irrelevant_chunks_count = 0
            missed_chunks_count = 0
            
            # æ–¹æ³•1: ä»raw_resultsçš„detailsä¸­æå–
            if raw_results and isinstance(raw_results, dict):
                if 'details' in raw_results:
                    details_list = raw_results['details']
                    total_samples = len(details_list)
                    irrelevant_chunks_count = sum(len(sample.get('irrelevant_chunks', [])) for sample in details_list)
                    missed_chunks_count = sum(len(sample.get('missed_chunks', [])) for sample in details_list)
                else:
                    # å¦‚æœæ²¡æœ‰detailsï¼Œå°è¯•ä»å…¶ä»–å­—æ®µè·å–
                    total_samples = raw_results.get('total_samples', 0)
                    irrelevant_chunks_count = raw_results.get('irrelevant_chunks_count', 0)
                    missed_chunks_count = raw_results.get('missed_chunks_count', 0)
            
            # æ–¹æ³•2: å¦‚æœraw_resultsä¸­æ²¡æœ‰æ•°æ®ï¼Œå°è¯•ä»åŸå§‹æ•°æ®é‡æ–°è®¡ç®—
            if total_samples == 0:
                print("ğŸ“Š ä»åŸå§‹æ•°æ®é‡æ–°è®¡ç®—Ragasç»Ÿè®¡æ•°æ®...")
                try:
                    # é‡æ–°åŠ è½½æ•°æ®è¿›è¡Œåˆ†æ
                    from read_chuck import EvaluationConfig, DataLoader, TextProcessor
                    
                    config = EvaluationConfig(
                        api_key=os.getenv("QWEN_API_KEY", ""),
                        api_base=os.getenv("QWEN_API_BASE", ""),
                        model_name=os.getenv("QWEN_MODEL_NAME", "qwen-plus"),
                        embedding_model=os.getenv("QWEN_EMBEDDING_MODEL", "text-embedding-v1"),
                        excel_file_path=os.getenv("EXCEL_FILE_PATH", "standardDataset/standardDataset.xlsx")
                    )
                    
                    data_loader = DataLoader(config)
                    text_processor = TextProcessor(config)
                    
                    df = data_loader.load_excel_data()
                    if df is not None:
                        df = text_processor.parse_context_columns(df)
                        
                        # è¿‡æ»¤ç©ºè¡Œæ•°æ®
                        filtered_rows = []
                        for i in range(len(df)):
                            retrieved_contexts = df['retrieved_contexts'].iloc[i]
                            reference_contexts = df['reference_contexts'].iloc[i]
                            user_input = df['user_input'].iloc[i] if 'user_input' in df.columns else ""
                            response = df['response'].iloc[i] if 'response' in df.columns else ""
                            
                            if not text_processor.is_empty_row_data(retrieved_contexts, reference_contexts, user_input, response):
                                filtered_rows.append(i)
                        
                        # ä½¿ç”¨è¿‡æ»¤åçš„æ•°æ®
                        df = df.iloc[filtered_rows].copy()
                        total_samples = len(df)
                        
                        # åˆ†ææ¯ä¸ªæ ·æœ¬çš„åˆ†å—æƒ…å†µ
                        for idx, row in df.iterrows():
                            retrieved_contexts = row['retrieved_contexts']
                            reference_contexts = row['reference_contexts']
                            
                            if not retrieved_contexts or not reference_contexts:
                                continue
                            
                            # åˆ†æä¸ç›¸å…³åˆ†å—ï¼ˆç®€åŒ–ç‰ˆæœ¬ï¼‰
                            for retrieved_chunk in retrieved_contexts:
                                is_relevant = False
                                for ref_chunk in reference_contexts:
                                    # ç®€å•çš„å…³é”®è¯åŒ¹é…
                                    retrieved_words = set(str(retrieved_chunk).lower().split())
                                    ref_words = set(str(ref_chunk).lower().split())
                                    overlap = len(retrieved_words.intersection(ref_words))
                                    if overlap > 3:  # è‡³å°‘3ä¸ªè¯é‡å 
                                        is_relevant = True
                                        break
                                
                                if not is_relevant:
                                    irrelevant_chunks_count += 1
                            
                            # åˆ†ææœªå¬å›åˆ†å—ï¼ˆç®€åŒ–ç‰ˆæœ¬ï¼‰
                            for ref_chunk in reference_contexts:
                                is_retrieved = False
                                for retrieved_chunk in retrieved_contexts:
                                    retrieved_words = set(str(retrieved_chunk).lower().split())
                                    ref_words = set(str(ref_chunk).lower().split())
                                    overlap = len(retrieved_words.intersection(ref_words))
                                    if overlap > 3:
                                        is_retrieved = True
                                        break
                                
                                if not is_retrieved:
                                    missed_chunks_count += 1
                        
                        print(f"âœ… é‡æ–°è®¡ç®—å®Œæˆ: total_samples={total_samples}, irrelevant_chunks={irrelevant_chunks_count}, missed_chunks={missed_chunks_count}")
                        
                except Exception as e:
                    print(f"âš ï¸ é‡æ–°è®¡ç®—ç»Ÿè®¡æ•°æ®å¤±è´¥: {e}")
                    # ä½¿ç”¨é»˜è®¤å€¼
                    total_samples = 1
                    irrelevant_chunks_count = 0
                    missed_chunks_count = 0
            
            # ç¡®ä¿è‡³å°‘æœ‰ä¸€ä¸ªæ ·æœ¬
            if total_samples == 0:
                total_samples = 1
            
            return {
                'total_samples': total_samples,
                'irrelevant_chunks_count': irrelevant_chunks_count,
                'missed_chunks_count': missed_chunks_count
            }
            
        except Exception as e:
            print(f"âŒ æå–Ragasç»Ÿè®¡æ•°æ®å¤±è´¥: {e}")
            return {
                'total_samples': 1,
                'irrelevant_chunks_count': 0,
                'missed_chunks_count': 0
            }
    
    @staticmethod
    def save_ragas_result(results: Dict[str, Any], description: str = "") -> Optional[int]:
        """ä¿å­˜Ragasè¯„ä¼°ç»“æœåˆ°ç‹¬ç«‹çš„ragas_evaluationsè¡¨"""
        try:
            with get_db_session() as session:
                # æå–ç»Ÿè®¡æ•°æ®
                stats = DatabaseService.extract_ragas_statistics(results)
                total_samples = stats['total_samples']
                irrelevant_chunks_count = stats['irrelevant_chunks_count']
                missed_chunks_count = stats['missed_chunks_count']
                
                # æå–æŒ‡æ ‡æ•°æ®
                context_precision = results.get('context_precision', 0)
                context_recall = results.get('context_recall', 0)
                faithfulness = results.get('faithfulness', 0)
                answer_relevancy = results.get('answer_relevancy', 0)
                context_entity_recall = results.get('context_entity_recall', 0)
                context_relevance = results.get('context_relevance', 0)
                answer_correctness = results.get('answer_correctness', 0)
                answer_similarity = results.get('answer_similarity', 0)
                
                # æ„å»ºSQLæŸ¥è¯¢ï¼Œå…¼å®¹MySQLå’ŒSQLite
                sql = """
                INSERT INTO ragas_evaluations 
                (evaluation_time, description, context_precision, context_recall, 
                 faithfulness, answer_relevancy, context_entity_recall, context_relevance,
                 answer_correctness, answer_similarity, total_samples, 
                 irrelevant_chunks_count, missed_chunks_count)
                VALUES (CURRENT_TIMESTAMP, :description, :context_precision, :context_recall, 
                 :faithfulness, :answer_relevancy, :context_entity_recall, :context_relevance,
                 :answer_correctness, :answer_similarity, :total_samples, 
                 :irrelevant_chunks_count, :missed_chunks_count)
                """
                
                result = session.execute(text(sql), {
                    'description': description,
                    'context_precision': context_precision,
                    'context_recall': context_recall,
                    'faithfulness': faithfulness,
                    'answer_relevancy': answer_relevancy,
                    'context_entity_recall': context_entity_recall,
                    'context_relevance': context_relevance,
                    'answer_correctness': answer_correctness,
                    'answer_similarity': answer_similarity,
                    'total_samples': total_samples,
                    'irrelevant_chunks_count': irrelevant_chunks_count,
                    'missed_chunks_count': missed_chunks_count
                })
                session.commit()
                
                # è·å–æ’å…¥çš„ID
                # å¯¹äºSQLAlchemyï¼Œæˆ‘ä»¬éœ€è¦é‡æ–°æŸ¥è¯¢è·å–ID
                try:
                    # MySQL
                    id_result = session.execute(text("SELECT LAST_INSERT_ID()"))
                except:
                    # SQLite
                    id_result = session.execute(text("SELECT last_insert_rowid()"))
                row = id_result.fetchone()
                return row[0] if row else None
        except Exception as e:
            print(f"ä¿å­˜Ragasç»“æœå¤±è´¥: {e}")
            import traceback
            traceback.print_exc()
            return None
    
    @staticmethod
    def get_evaluation_history(limit: int = 50) -> List[Dict[str, Any]]:
        """è·å–è¯„ä¼°å†å²è®°å½•ï¼ˆä»ç‹¬ç«‹çš„è¡¨ä¸­è·å–ï¼‰"""
        try:
            with get_db_session() as session:
                # è·å–BM25è¯„ä¼°å†å²
                bm25_sql = """
                SELECT id, evaluation_time, description, context_precision, context_recall,
                       f1_score, mrr, map, ndcg, total_samples, irrelevant_chunks_count, missed_chunks_count,
                       'BM25' as evaluation_type
                FROM bm25_evaluations 
                ORDER BY evaluation_time DESC 
                LIMIT :limit
                """
                
                # è·å–Ragasè¯„ä¼°å†å²
                ragas_sql = """
                SELECT id, evaluation_time, description, context_precision, context_recall,
                       faithfulness, answer_relevancy, context_entity_recall, context_relevance,
                       answer_correctness, answer_similarity, total_samples, 
                       irrelevant_chunks_count, missed_chunks_count,
                       'RAGAS' as evaluation_type
                FROM ragas_evaluations 
                ORDER BY evaluation_time DESC 
                LIMIT :limit
                """
                
                bm25_result = session.execute(text(bm25_sql), {'limit': limit})
                ragas_result = session.execute(text(ragas_sql), {'limit': limit})
                
                bm25_results = bm25_result.fetchall()
                ragas_results = ragas_result.fetchall()
                
                # åˆå¹¶ç»“æœå¹¶æŒ‰æ—¶é—´æ’åº
                all_results = []
                for row in bm25_results:
                    # å°†Rowå¯¹è±¡è½¬æ¢ä¸ºå­—å…¸
                    if hasattr(row, '_asdict'):
                        all_results.append(row._asdict())
                    else:
                        all_results.append(dict(row))
                for row in ragas_results:
                    # å°†Rowå¯¹è±¡è½¬æ¢ä¸ºå­—å…¸
                    if hasattr(row, '_asdict'):
                        all_results.append(row._asdict())
                    else:
                        all_results.append(dict(row))
                
                # æŒ‰æ—¶é—´æ’åº
                all_results.sort(key=lambda x: x['evaluation_time'], reverse=True)
                return all_results[:limit]
        except Exception as e:
            print(f"è·å–è¯„ä¼°å†å²å¤±è´¥: {e}")
            import traceback
            traceback.print_exc()
            return []
    
    @staticmethod
    def get_evaluation_by_id(evaluation_id: int) -> Optional[Dict[str, Any]]:
        """æ ¹æ®IDè·å–è¯„ä¼°ç»“æœï¼ˆä»ç‹¬ç«‹çš„è¡¨ä¸­è·å–ï¼‰"""
        try:
            with get_db_session() as session:
                # å…ˆå°è¯•åœ¨bm25_evaluationsè¡¨ä¸­æŸ¥æ‰¾
                bm25_sql = """
                SELECT id, evaluation_time, description, context_precision, context_recall,
                       f1_score, mrr, map, ndcg, total_samples, irrelevant_chunks_count, missed_chunks_count,
                       'BM25' as evaluation_type
                FROM bm25_evaluations 
                WHERE id = :id
                """
                
                # å†å°è¯•åœ¨ragas_evaluationsè¡¨ä¸­æŸ¥æ‰¾
                ragas_sql = """
                SELECT id, evaluation_time, description, context_precision, context_recall,
                       faithfulness, answer_relevancy, context_entity_recall, context_relevance,
                       answer_correctness, answer_similarity, total_samples, 
                       irrelevant_chunks_count, missed_chunks_count,
                       'RAGAS' as evaluation_type
                FROM ragas_evaluations 
                WHERE id = :id
                """
                
                bm25_result = session.execute(text(bm25_sql), {'id': evaluation_id})
                bm25_row = bm25_result.fetchone()
                if bm25_row:
                    return dict(bm25_row)
                
                ragas_result = session.execute(text(ragas_sql), {'id': evaluation_id})
                ragas_row = ragas_result.fetchone()
                if ragas_row:
                    return dict(ragas_row)
                
                return None
        except Exception as e:
            print(f"è·å–è¯„ä¼°ç»“æœå¤±è´¥: {e}")
            import traceback
            traceback.print_exc()
            return None
    
    @staticmethod
    def get_statistics() -> Dict[str, Any]:
        """è·å–è¯„ä¼°ç»Ÿè®¡ä¿¡æ¯ï¼ˆä»ç‹¬ç«‹çš„è¡¨ä¸­è·å–ï¼‰"""
        try:
            with get_db_session() as session:
                # ç»Ÿè®¡BM25è¯„ä¼°æ•°é‡
                bm25_sql = "SELECT COUNT(*) FROM bm25_evaluations"
                ragas_sql = "SELECT COUNT(*) FROM ragas_evaluations"
                
                bm25_result = session.execute(text(bm25_sql))
                ragas_result = session.execute(text(ragas_sql))
                
                bm25_count = bm25_result.scalar() or 0
                ragas_count = ragas_result.scalar() or 0
                
                # è·å–æœ€æ–°è¯„ä¼°æ—¶é—´
                latest_sql = """
                SELECT MAX(evaluation_time) FROM (
                    SELECT evaluation_time FROM bm25_evaluations
                    UNION ALL
                    SELECT evaluation_time FROM ragas_evaluations
                ) as all_evaluations
                """
                latest_result = session.execute(text(latest_sql))
                latest_evaluation = latest_result.scalar()
                
                # å¤„ç†æ—¥æœŸæ ¼å¼ï¼ˆSQLiteè¿”å›å­—ç¬¦ä¸²ï¼ŒMySQLè¿”å›datetimeå¯¹è±¡ï¼‰
                latest_evaluation_time = None
                if latest_evaluation:
                    if hasattr(latest_evaluation, 'isoformat'):
                        # MySQL datetimeå¯¹è±¡
                        latest_evaluation_time = latest_evaluation.isoformat()
                    else:
                        # SQLiteå­—ç¬¦ä¸²
                        latest_evaluation_time = str(latest_evaluation)
                
                return {
                    'total_evaluations': bm25_count + ragas_count,
                    'bm25_evaluations': bm25_count,
                    'ragas_evaluations': ragas_count,
                    'latest_evaluation_time': latest_evaluation_time
                }
        except Exception as e:
            print(f"è·å–ç»Ÿè®¡ä¿¡æ¯å¤±è´¥: {e}")
            import traceback
            traceback.print_exc()
            return {
                'total_evaluations': 0,
                'bm25_evaluations': 0,
                'ragas_evaluations': 0,
                'latest_evaluation_time': None
            }

# å†å²æ•°æ®åˆ†æç›¸å…³å‡½æ•°ï¼ˆä¿®æ”¹ä¸ºä½¿ç”¨ç‹¬ç«‹çš„è¡¨ï¼‰
def get_evaluation_history(evaluation_type: str, metric: str) -> List[Dict[str, Any]]:
    """è·å–æŒ‡å®šè¯„ä¼°ç±»å‹å’ŒæŒ‡æ ‡çš„å†å²æ•°æ®ï¼ˆä»ç‹¬ç«‹çš„è¡¨ä¸­è·å–ï¼‰"""
    try:
        with get_db_session() as session:
            if evaluation_type.upper() == 'BM25':
                # ç‰¹æ®Šå¤„ç†mapæŒ‡æ ‡çš„åˆ—å
                actual_metric = 'map' if metric == 'map_score' else metric
                # æ„å»ºSQLæŸ¥è¯¢ï¼Œå…¼å®¹MySQLå’ŒSQLite
                sql = f"""
                    SELECT id, evaluation_time, {actual_metric}
                    FROM bm25_evaluations
                    WHERE {actual_metric} IS NOT NULL 
                    AND {actual_metric} > 0
                    ORDER BY evaluation_time ASC
                """
            else:  # RAGAS
                # æ„å»ºSQLæŸ¥è¯¢ï¼Œå…¼å®¹MySQLå’ŒSQLite
                sql = f"""
                    SELECT id, evaluation_time, {metric}
                    FROM ragas_evaluations
                    WHERE {metric} IS NOT NULL 
                    AND {metric} > 0
                    ORDER BY evaluation_time ASC
                """
            
            result = session.execute(text(sql))
            rows = result.fetchall()
            
            # æ ¼å¼åŒ–æ•°æ®
            data = []
            for row in rows:
                # å¤„ç†æ—¥æœŸæ ¼å¼ï¼ˆSQLiteè¿”å›å­—ç¬¦ä¸²ï¼ŒMySQLè¿”å›datetimeå¯¹è±¡ï¼‰
                evaluation_time = row[1]
                if evaluation_time:
                    if hasattr(evaluation_time, 'isoformat'):
                        # MySQL datetimeå¯¹è±¡
                        evaluation_time_str = evaluation_time.isoformat()
                    else:
                        # SQLiteå­—ç¬¦ä¸²
                        evaluation_time_str = str(evaluation_time)
                else:
                    evaluation_time_str = None
                
                data.append({
                    'created_at': evaluation_time_str,
                    'value': float(row[2]) if row[2] else None,
                    'id': row[0]
                })
            
            return data
    except Exception as e:
        print(f"è·å–{evaluation_type} {metric}å†å²æ•°æ®å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
        return []

def get_evaluation_stats() -> Dict[str, Any]:
    """è·å–è¯„ä¼°ç»Ÿè®¡æ¦‚è§ˆï¼ˆä»ç‹¬ç«‹çš„è¡¨ä¸­è·å–ï¼‰"""
    try:
        with get_db_session() as session:
            # è·å–BM25å’ŒRAGASçš„æ€»è¯„ä¼°æ¬¡æ•°
            bm25_count_sql = "SELECT COUNT(*) FROM bm25_evaluations"
            ragas_count_sql = "SELECT COUNT(*) FROM ragas_evaluations"
            
            bm25_count = session.execute(text(bm25_count_sql)).scalar() or 0
            ragas_count = session.execute(text(ragas_count_sql)).scalar() or 0
            total_evaluations = bm25_count + ragas_count
            
            # è®¡ç®—å¹³å‡å‡†ç¡®ç‡
            bm25_precision_sql = """
                SELECT AVG(context_precision) 
                FROM bm25_evaluations 
                WHERE context_precision IS NOT NULL
            """
            ragas_precision_sql = """
                SELECT AVG(context_precision) 
                FROM ragas_evaluations 
                WHERE context_precision IS NOT NULL
            """
            
            bm25_avg_precision = session.execute(text(bm25_precision_sql)).scalar() or 0
            ragas_avg_precision = session.execute(text(ragas_precision_sql)).scalar() or 0
            
            # è®¡ç®—åŠ æƒå¹³å‡å‡†ç¡®ç‡
            if bm25_count > 0 and ragas_count > 0:
                avg_precision = (bm25_avg_precision * bm25_count + ragas_avg_precision * ragas_count) / total_evaluations
            elif bm25_count > 0:
                avg_precision = bm25_avg_precision
            elif ragas_count > 0:
                avg_precision = ragas_avg_precision
            else:
                avg_precision = 0
            
            # è®¡ç®—å¹³å‡å¬å›ç‡
            bm25_recall_sql = """
                SELECT AVG(context_recall) 
                FROM bm25_evaluations 
                WHERE context_recall IS NOT NULL
            """
            ragas_recall_sql = """
                SELECT AVG(context_recall) 
                FROM ragas_evaluations 
                WHERE context_recall IS NOT NULL
            """
            
            bm25_avg_recall = session.execute(text(bm25_recall_sql)).scalar() or 0
            ragas_avg_recall = session.execute(text(ragas_recall_sql)).scalar() or 0
            
            # è®¡ç®—åŠ æƒå¹³å‡å¬å›ç‡
            if bm25_count > 0 and ragas_count > 0:
                avg_recall = (bm25_avg_recall * bm25_count + ragas_avg_recall * ragas_count) / total_evaluations
            elif bm25_count > 0:
                avg_recall = bm25_avg_recall
            elif ragas_count > 0:
                avg_recall = ragas_avg_recall
            else:
                avg_recall = 0
            
            # è·å–æœ€æ–°æ›´æ–°æ—¶é—´ï¼ˆå…¼å®¹MySQLå’ŒSQLiteï¼‰
            latest_sql = """
                SELECT MAX(evaluation_time) as latest_time 
                FROM (
                    SELECT evaluation_time FROM bm25_evaluations
                    UNION ALL
                    SELECT evaluation_time FROM ragas_evaluations
                ) as all_evaluations
            """
            latest_time = session.execute(text(latest_sql)).scalar()
            
            # å¤„ç†æ—¥æœŸæ ¼å¼
            latest_update = None
            if latest_time:
                if hasattr(latest_time, 'isoformat'):
                    # MySQL datetimeå¯¹è±¡
                    latest_update = latest_time.isoformat()
                else:
                    # SQLiteå­—ç¬¦ä¸²
                    latest_update = str(latest_time)
            
            return {
                'total_evaluations': total_evaluations,
                'avg_precision': float(avg_precision) if avg_precision else 0,
                'avg_recall': float(avg_recall) if avg_recall else 0,
                'latest_update': latest_update
            }
    except Exception as e:
        print(f"è·å–è¯„ä¼°ç»Ÿè®¡å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
        return {
            'total_evaluations': 0,
            'avg_precision': 0,
            'avg_recall': 0,
            'latest_update': None
        }