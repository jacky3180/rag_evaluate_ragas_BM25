"""
åŸºäºBM25ç®—æ³•çš„RAGè¯„ä¼°ç³»ç»Ÿ
ç”¨äºè®¡ç®—åˆ†å—çº§åˆ«çš„å¬å›ç‡å’Œå‡†ç¡®ç‡

åŠŸèƒ½ï¼š
1. BM25ç®—æ³•è®¡ç®—æ–‡æœ¬ç›¸ä¼¼åº¦
2. è®¡ç®—Precisionå’ŒRecallæŒ‡æ ‡
3. åˆ†æä¸ç›¸å…³å’Œæœªå¬å›çš„åˆ†å—
4. ç›¸å…³æ€§è¯„åˆ†ç³»ç»Ÿ
"""

import os
import json
import pandas as pd
import numpy as np
import re
from typing import Dict, List, Optional, Any, Tuple
from config import debug_print, verbose_print, info_print, error_print, QUIET_MODE
from collections import Counter
import math
from read_chuck import EvaluationConfig, DataLoader, TextProcessor

class BM25:
    """BM25ç®—æ³•å®ç°"""
    
    def __init__(self, k1=1.5, b=0.75):
        """
        åˆå§‹åŒ–BM25ç®—æ³•
        
        Args:
            k1: æ§åˆ¶è¯é¢‘é¥±å’Œåº¦çš„å‚æ•°
            b: æ§åˆ¶æ–‡æ¡£é•¿åº¦å½’ä¸€åŒ–çš„å‚æ•°
        """
        self.k1 = k1
        self.b = b
        self.corpus = []
        self.corpus_size = 0
        self.doc_freqs = []
        self.idf = {}
        self.doc_len = []
        self.avgdl = 0
    
    def fit(self, corpus: List[str]):
        """
        è®­ç»ƒBM25æ¨¡å‹
        
        Args:
            corpus: æ–‡æ¡£è¯­æ–™åº“
        """
        self.corpus = [self._tokenize(doc) for doc in corpus]
        self.corpus_size = len(self.corpus)
        
        # è®¡ç®—æ–‡æ¡£é•¿åº¦
        self.doc_len = [len(doc) for doc in self.corpus]
        self.avgdl = sum(self.doc_len) / self.corpus_size if self.corpus_size > 0 else 0
        
        # è®¡ç®—è¯é¢‘å’Œæ–‡æ¡£é¢‘ç‡
        self.doc_freqs = []
        df = {}
        
        for doc in self.corpus:
            frequencies = Counter(doc)
            self.doc_freqs.append(frequencies)
            
            for word in frequencies:
                df[word] = df.get(word, 0) + 1
        
        # è®¡ç®—IDF
        for word, freq in df.items():
            self.idf[word] = math.log((self.corpus_size - freq + 0.5) / (freq + 0.5))
    
    def _tokenize(self, text: str) -> List[str]:
        """
        æ”¯æŒä¸­æ–‡çš„åˆ†è¯å‡½æ•°
        
        Args:
            text: è¾“å…¥æ–‡æœ¬
            
        Returns:
            List[str]: åˆ†è¯ç»“æœ
        """
        if not text:
            return []
        
        # è½¬æ¢ä¸ºå°å†™
        text = text.lower()
        
        # æ›´ç»†ç²’åº¦çš„ä¸­æ–‡åˆ†è¯
        # 1. æå–ä¸­æ–‡å­—ç¬¦ï¼ˆåŒ…æ‹¬å•å­—ï¼‰
        chinese_chars = re.findall(r'[\u4e00-\u9fff]', text)
        
        # 2. æå–è‹±æ–‡å•è¯
        english_words = re.findall(r'[a-zA-Z]+', text)
        
        # 3. æå–æ•°å­—
        numbers = re.findall(r'\d+', text)
        
        # 4. æå–ä¹¦åï¼ˆã€Šã€‹ä¸­çš„å†…å®¹ï¼‰
        book_titles = re.findall(r'ã€Š([^ã€‹]+)ã€‹', text)
        
        # 5. æå–å¸¸è§çš„åŠ¨ä½œè¯å’Œå…³é”®è¯
        keywords = []
        action_words = ['åˆ†äº«', 'æ¨è', 'è¯»äº†', 'è¯»äº†æœ¬', 'æœ€è¿‘è¯»äº†', 'ä¹¦ç±', 'ä¹¦', 'è¯»ä¹¦', 'é˜…è¯»', 'è¯»åæ„Ÿ']
        for word in action_words:
            if word in text:
                keywords.append(word)
        
        # åˆå¹¶æ‰€æœ‰token
        tokens = chinese_chars + english_words + numbers + book_titles + keywords
        
        # å»é‡å¹¶è¿‡æ»¤æ‰é•¿åº¦å°äº1çš„token
        tokens = list(set([token for token in tokens if len(token) >= 1]))
        
        return tokens
    
    def score(self, query: str, doc_index: int) -> float:
        """
        è®¡ç®—æŸ¥è¯¢ä¸æ–‡æ¡£çš„BM25åˆ†æ•°
        
        Args:
            query: æŸ¥è¯¢æ–‡æœ¬
            doc_index: æ–‡æ¡£ç´¢å¼•
            
        Returns:
            float: BM25åˆ†æ•°
        """
        if doc_index >= len(self.corpus):
            return 0.0
        
        query_tokens = self._tokenize(query)
        doc_freqs = self.doc_freqs[doc_index]
        doc_len = self.doc_len[doc_index]
        
        score = 0.0
        for token in query_tokens:
            if token in doc_freqs:
                tf = doc_freqs[token]
                idf = self.idf.get(token, 0)
                
                # BM25å…¬å¼
                numerator = tf * (self.k1 + 1)
                denominator = tf + self.k1 * (1 - self.b + self.b * (doc_len / self.avgdl))
                score += idf * (numerator / denominator)
        
        return score
    
    def get_scores(self, query: str) -> List[float]:
        """
        è·å–æŸ¥è¯¢å¯¹æ‰€æœ‰æ–‡æ¡£çš„åˆ†æ•°
        
        Args:
            query: æŸ¥è¯¢æ–‡æœ¬
            
        Returns:
            List[float]: æ‰€æœ‰æ–‡æ¡£çš„åˆ†æ•°
        """
        return [self.score(query, i) for i in range(self.corpus_size)]

class BM25Evaluator:
    """åŸºäºBM25çš„RAGè¯„ä¼°å™¨"""
    
    def __init__(self, config: EvaluationConfig):
        self.config = config
        self.data_loader = DataLoader(config)
        self.text_processor = TextProcessor(config)
        self.bm25 = BM25()
        
        # ç›¸å…³æ€§é˜ˆå€¼
        self.relevance_thresholds = {
            0.0000: "å®Œå…¨ä¸ç›¸å…³",
            0.2500: "å°‘é‡ç›¸å…³ï¼Œä½†ä¿¡æ¯ä¸å®Œæ•´", 
            0.5000: "éƒ¨åˆ†ç›¸å…³ï¼Œæœ‰ä¸€å®šä»·å€¼",
            0.7500: "å¤§éƒ¨åˆ†ç›¸å…³ï¼ŒåŸºæœ¬æ»¡è¶³éœ€æ±‚",
            1.0000: "å®Œå…¨ç›¸å…³ï¼Œå®Œç¾åŒ¹é…"
        }
    
    def load_and_process_data(self) -> Optional[pd.DataFrame]:
        """
        åŠ è½½å’Œå¤„ç†æ•°æ®
        
        Returns:
            pd.DataFrame: å¤„ç†åçš„æ•°æ®
        """
        info_print("ğŸ“– åŠ è½½æ•°æ®...")
        
        # åŠ è½½æ•°æ®
        df = self.data_loader.load_excel_data()
        if df is None:
            return None
        
        # éªŒè¯æ•°æ®
        if not self.data_loader.validate_data(df):
            return None
        
        # å¤„ç†ä¸Šä¸‹æ–‡åˆ—ï¼ˆé¿å…é‡å¤å¤„ç†ï¼‰
        # æ£€æŸ¥æ˜¯å¦éœ€è¦å¤„ç†ï¼šå¦‚æœç¬¬ä¸€ä¸ªå…ƒç´ æ˜¯å­—ç¬¦ä¸²ï¼Œåˆ™éœ€è¦å¤„ç†
        first_retrieved = df['retrieved_contexts'].iloc[0]
        if isinstance(first_retrieved, str):
            df = self.text_processor.parse_context_columns(df)
        
        info_print(f"âœ… æˆåŠŸåŠ è½½ {len(df)} è¡Œæ•°æ®")
        return df
    
    def calculate_relevance_score(self, retrieved_chunk: str, reference_chunk: str) -> float:
        """
        è®¡ç®—æ£€ç´¢åˆ†å—ä¸å‚è€ƒåˆ†å—çš„ç›¸å…³æ€§åˆ†æ•°
        ä½¿ç”¨æ™ºèƒ½ç›¸ä¼¼åº¦ç®—æ³•æ›¿ä»£BM25ï¼Œç¡®ä¿å‡†ç¡®æ€§
        
        Args:
            retrieved_chunk: æ£€ç´¢åˆ°çš„åˆ†å—
            reference_chunk: å‚è€ƒåˆ†å—
            
        Returns:
            float: ç›¸å…³æ€§åˆ†æ•° (0-1)
        """
        if not retrieved_chunk or not reference_chunk:
            return 0.0
        
        # ä½¿ç”¨æ™ºèƒ½ç›¸ä¼¼åº¦ç®—æ³•ï¼ˆä¸Ragasè¯„ä¼°æ˜ç»†ä¿æŒä¸€è‡´ï¼‰
        from app import calculate_text_similarity
        similarity = calculate_text_similarity(retrieved_chunk, reference_chunk)
        
        return similarity
    
    def _check_semantic_containment(self, retrieved_chunk: str, reference_chunk: str, threshold: float) -> bool:
        """
        æ£€æŸ¥æ˜¯å¦æ˜¯è¯­ä¹‰åŒ…å«æƒ…å†µ
        
        Args:
            retrieved_chunk: æ£€ç´¢åˆ°çš„åˆ†å—
            reference_chunk: å‚è€ƒåˆ†å—
            threshold: è¯­ä¹‰åŒ…å«é˜ˆå€¼
            
        Returns:
            bool: æ˜¯å¦æ˜¯è¯­ä¹‰åŒ…å«
        """
        if not retrieved_chunk or not reference_chunk:
            return False
        
        # æ–‡æœ¬é¢„å¤„ç†
        import re
        
        def clean_text(text):
            # ç§»é™¤æ ‡ç‚¹ç¬¦å·å’Œç‰¹æ®Šå­—ç¬¦ï¼Œä¿ç•™ä¸­æ–‡ã€è‹±æ–‡ã€æ•°å­—
            cleaned = re.sub(r'[^\w\s\u4e00-\u9fff]', ' ', text)
            # ç§»é™¤å¤šä½™ç©ºæ ¼
            cleaned = re.sub(r'\s+', ' ', cleaned).strip()
            return cleaned.lower()
        
        clean_retrieved = clean_text(retrieved_chunk)
        clean_reference = clean_text(reference_chunk)
        
        # åˆ†è¯
        words_retrieved = set(clean_retrieved.split())
        words_reference = set(clean_reference.split())
        
        if len(words_retrieved) == 0 or len(words_reference) == 0:
            return False
        
        # è®¡ç®—è¯­ä¹‰åŒ…å«åº¦
        if len(words_retrieved) <= len(words_reference):
            # retrieved_chunkæ˜¯è¾ƒçŸ­çš„ï¼Œè®¡ç®—åœ¨reference_chunkä¸­çš„åŒ…å«åº¦
            contained_words = words_retrieved.intersection(words_reference)
            semantic_containment = len(contained_words) / len(words_retrieved)
        else:
            # reference_chunkæ˜¯è¾ƒçŸ­çš„ï¼Œè®¡ç®—åœ¨retrieved_chunkä¸­çš„åŒ…å«åº¦
            contained_words = words_reference.intersection(words_retrieved)
            semantic_containment = len(contained_words) / len(words_reference)
        
        return semantic_containment >= threshold
    
    def evaluate_precision_recall(self, df: pd.DataFrame) -> Dict[str, Any]:
        """
        è®¡ç®—Precisionå’ŒRecallæŒ‡æ ‡
        åŸºäºBM25ç®—æ³•åˆ¤æ–­æ£€ç´¢åˆ†å—ä¸æ ‡å‡†ç­”æ¡ˆåˆ†å—çš„è¯­ä¹‰ä¸€è‡´æ€§
        
        Args:
            df: å¤„ç†åçš„æ•°æ®
            
        Returns:
            Dict[str, Any]: è¯„ä¼°ç»“æœ
        """
        info_print("ğŸ” å¼€å§‹BM25è¯„ä¼°...")
        info_print("ğŸ“‹ è¯„ä¼°é€»è¾‘:")
        info_print("  â€¢ Precision = å®Œæ•´å«æœ‰ç›¸å…³ä¿¡æ¯çš„åˆ†å—æ•°ï¼ˆè¯­ä¹‰å¾—åˆ†ï¼‰ / retrieved_contextsåˆ†å—æ•°")
        info_print("  â€¢ Recall = å®Œæ•´å«æœ‰ç›¸å…³ä¿¡æ¯çš„åˆ†å—æ•°ï¼ˆè¯­ä¹‰å¾—åˆ†ï¼‰ / reference_contextsåˆ†å—æ•°")
        info_print("  â€¢ å®Œæ•´å«æœ‰ç›¸å…³ä¿¡æ¯çš„åˆ†å—æ•° = æ‰€æœ‰ç›¸å…³åˆ†å—çš„è¯­ä¹‰ç›¸ä¼¼åº¦å¾—åˆ†ä¹‹å’Œ")
        similarity_threshold = float(os.getenv("SIMILARITY_THRESHOLD", "0.5"))
        info_print(f"  â€¢ ç›¸å…³æ€§åˆ¤æ–­: æ£€ç´¢åˆ†å—ä¸å‚è€ƒåˆ†å—çš„è¯­ä¹‰ç›¸ä¼¼åº¦ > {similarity_threshold}")
        info_print()
        
        results = {
            'precision_scores': [],
            'recall_scores': [],
            'irrelevant_chunks': [],  # ä¸ç›¸å…³çš„æ£€ç´¢åˆ†å—
            'missed_chunks': [],      # æœªå¬å›çš„å‚è€ƒåˆ†å—
            'relevant_chunks': [],    # ç›¸å…³çš„æ£€ç´¢åˆ†å—
            'detailed_results': []
        }
        
        for idx, row in df.iterrows():
            info_print(f"å¤„ç†ç¬¬ {idx + 1}/{len(df)} è¡Œ...")
            
            user_input = str(row['user_input']) if pd.notna(row['user_input']) else ""
            retrieved_contexts = row['retrieved_contexts']
            reference_contexts = row['reference_contexts']
            
            if not retrieved_contexts or not reference_contexts:
                info_print(f"  è·³è¿‡è¡Œ {idx + 1}: ç¼ºå°‘ä¸Šä¸‹æ–‡æ•°æ®")
                continue
            
            # è®¡ç®—æ£€ç´¢åˆ†å—ä¸å‚è€ƒåˆ†å—çš„è¯­ä¹‰ç›¸ä¼¼åº¦çŸ©é˜µ
            similarity_matrix = []
            for retrieved_chunk in retrieved_contexts:
                chunk_similarities = []
                for reference_chunk in reference_contexts:
                    similarity = self.calculate_relevance_score(retrieved_chunk, reference_chunk)
                    chunk_similarities.append(similarity)
                similarity_matrix.append(chunk_similarities)
            
            # æ‰¾å‡ºæ¯ä¸ªæ£€ç´¢åˆ†å—çš„æœ€ä½³åŒ¹é…å‚è€ƒåˆ†å—
            relevant_retrieved = []  # è¢«åˆ¤å®šä¸ºç›¸å…³çš„æ£€ç´¢åˆ†å—
            matched_references = set()  # å·²è¢«åŒ¹é…çš„å‚è€ƒåˆ†å—ç´¢å¼•
            total_relevance_score = 0.0  # æ€»ç›¸å…³æ€§å¾—åˆ†
            
            for i, retrieved_chunk in enumerate(retrieved_contexts):
                max_similarity = max(similarity_matrix[i]) if similarity_matrix[i] else 0
                best_ref_idx = similarity_matrix[i].index(max_similarity) if similarity_matrix[i] else -1
                
                # åˆ¤æ–­æ˜¯å¦ç›¸å…³ï¼ˆä»ç¯å¢ƒå˜é‡è¯»å–é˜ˆå€¼ï¼‰
                similarity_threshold = float(os.getenv("SIMILARITY_THRESHOLD", "0.5"))
                if max_similarity > similarity_threshold:
                    # æ£€æŸ¥æ˜¯å¦æ˜¯è¯­ä¹‰åŒ…å«æƒ…å†µ
                    semantic_containment_threshold = float(os.getenv("SEMANTIC_CONTAINMENT_THRESHOLD", "0.9"))
                    is_semantic_containment = self._check_semantic_containment(retrieved_chunk, reference_contexts[best_ref_idx], semantic_containment_threshold)
                    
                    relevant_chunk_info = {
                        'retrieved_chunk': retrieved_chunk,
                        'reference_chunk': reference_contexts[best_ref_idx],
                        'relevance_score': max_similarity,
                        'row_index': idx,
                        'retrieved_idx': i,
                        'reference_idx': best_ref_idx,
                        'user_input': user_input,
                        'is_semantic_containment': is_semantic_containment,
                        'semantic_containment_threshold': semantic_containment_threshold
                    }
                    relevant_retrieved.append(relevant_chunk_info)
                    # ç´¯åŠ ç›¸å…³æ€§å¾—åˆ†
                    total_relevance_score += max_similarity
                    # æ·»åŠ åˆ°results['relevant_chunks']ä¸­ï¼Œä¸irrelevant_chunksä¿æŒä¸€è‡´
                    results['relevant_chunks'].append(relevant_chunk_info)
                    matched_references.add(best_ref_idx)
                else:
                    # ä¸ç›¸å…³çš„æ£€ç´¢åˆ†å—
                    results['irrelevant_chunks'].append({
                        'retrieved_chunk': retrieved_chunk,
                        'max_relevance': max_similarity,
                        'row_index': idx,
                        'user_input': user_input,
                        'retrieved_idx': i
                    })
            
            # æ‰¾å‡ºæœªè¢«å¬å›çš„å‚è€ƒåˆ†å—
            # æœªå¬å›åˆ†å—ï¼šreference_contextsä¸­å­˜åœ¨åˆ†å—ï¼Œåœ¨retrieved_contextsä¸­çš„åˆ†å—æ²¡æœ‰å­˜åœ¨ï¼ˆå³æ‰¾ä¸åˆ°ç›¸ä¼¼åº¦å¤§äºé˜ˆå€¼çš„ï¼‰
            for j, reference_chunk in enumerate(reference_contexts):
                # æ‰¾åˆ°è¯¥å‚è€ƒåˆ†å—ä¸æ‰€æœ‰æ£€ç´¢åˆ†å—çš„æœ€å¤§ç›¸ä¼¼åº¦
                max_similarity = 0
                best_retrieved_idx = -1
                for i in range(len(retrieved_contexts)):
                    if similarity_matrix[i][j] > max_similarity:
                        max_similarity = similarity_matrix[i][j]
                        best_retrieved_idx = i
                
                # å¦‚æœæœ€å¤§ç›¸ä¼¼åº¦å°äºç­‰äºé˜ˆå€¼ï¼Œè®¤ä¸ºæ˜¯æœªå¬å›åˆ†å—
                if max_similarity <= similarity_threshold:
                    results['missed_chunks'].append({
                        'reference_chunk': reference_chunk,
                        'max_relevance': max_similarity,
                        'row_index': idx,
                        'user_input': user_input,
                        'reference_idx': j,
                        'best_retrieved_idx': best_retrieved_idx
                    })
            
            # è®¡ç®—Precisionå’ŒRecall
            # Precision = å®Œæ•´å«æœ‰ç›¸å…³ä¿¡æ¯çš„åˆ†å—æ•°ï¼ˆè¯­ä¹‰å¾—åˆ†ï¼‰ / retrieved_contextsåˆ†å—æ•°
            # Recall = å®Œæ•´å«æœ‰ç›¸å…³ä¿¡æ¯çš„åˆ†å—æ•°ï¼ˆè¯­ä¹‰å¾—åˆ†ï¼‰ / reference_contextsåˆ†å—æ•°
            precision = total_relevance_score / len(retrieved_contexts) if retrieved_contexts else 0
            recall = total_relevance_score / len(reference_contexts) if reference_contexts else 0
            
            results['precision_scores'].append(precision)
            results['recall_scores'].append(recall)
            
            # è¯¦ç»†ç»“æœ
            results['detailed_results'].append({
                'row_index': idx,
                'user_input': user_input,
                'precision': precision,
                'recall': recall,
                'retrieved_count': len(retrieved_contexts),
                'reference_count': len(reference_contexts),
                'relevant_count': len(relevant_retrieved),
                'total_relevance_score': total_relevance_score,
                'matched_reference_count': len(matched_references),
                'relevant_chunks': relevant_retrieved,
                'similarity_matrix': similarity_matrix
            })
            
            info_print(f"  æ£€ç´¢åˆ†å—: {len(retrieved_contexts)}ä¸ª, å‚è€ƒåˆ†å—: {len(reference_contexts)}ä¸ª")
            info_print(f"  å«æœ‰ç›¸å…³ä¿¡æ¯çš„åˆ†å—: {len(relevant_retrieved)}ä¸ª, æ€»è¯­ä¹‰å¾—åˆ†: {total_relevance_score:.4f}, å¬å›åˆ†å—: {len(matched_references)}ä¸ª")
            info_print(f"  Precision: {precision:.4f}, Recall: {recall:.4f}")
        
        # è®¡ç®—å¹³å‡æŒ‡æ ‡
        results['avg_precision'] = np.mean(results['precision_scores']) if results['precision_scores'] else 0
        results['avg_recall'] = np.mean(results['recall_scores']) if results['recall_scores'] else 0
        results['avg_f1'] = 2 * (results['avg_precision'] * results['avg_recall']) / (results['avg_precision'] + results['avg_recall']) if (results['avg_precision'] + results['avg_recall']) > 0 else 0
        
        info_print(f"\nâœ… BM25è¯„ä¼°å®Œæˆ")
        info_print(f"ğŸ“Š å¹³å‡Precision: {results['avg_precision']:.4f}")
        info_print(f"ğŸ“Š å¹³å‡Recall: {results['avg_recall']:.4f}")
        info_print(f"ğŸ“Š å¹³å‡F1: {results['avg_f1']:.4f}")
        
        return results
    
    def print_sample_analysis(self, results: Dict[str, Any]):
        """
        æŒ‰æ ·æœ¬æ‰“å°ä¸ç›¸å…³å’Œæœªå¬å›çš„åˆ†å—åˆ†æ
        
        Args:
            results: è¯„ä¼°ç»“æœ
        """
        info_print("\n" + "=" * 80)
        info_print("ğŸ“Š åˆ†å—ä¿¡æ¯è¯¦ç»†åˆ†æ")
        info_print("=" * 80)
        
        # æŒ‰æ ·æœ¬ç»„ç»‡æ•°æ®
        sample_data = {}
        
        # ç»„ç»‡ä¸å«æœ‰ç›¸å…³ä¿¡æ¯çš„åˆ†å—
        for chunk_info in results['irrelevant_chunks']:
            row_idx = chunk_info['row_index']
            if row_idx not in sample_data:
                sample_data[row_idx] = {
                    'user_input': chunk_info['user_input'],
                    'irrelevant_chunks': [],
                    'missed_chunks': []
                }
            sample_data[row_idx]['irrelevant_chunks'].append(chunk_info)
        
        # ç»„ç»‡æœªå¬å›åˆ†å—
        for chunk_info in results['missed_chunks']:
            row_idx = chunk_info['row_index']
            if row_idx not in sample_data:
                sample_data[row_idx] = {
                    'user_input': chunk_info['user_input'],
                    'irrelevant_chunks': [],
                    'missed_chunks': []
                }
            sample_data[row_idx]['missed_chunks'].append(chunk_info)
        
        # æŒ‰æ ·æœ¬æ‰“å°
        for sample_idx, (row_idx, data) in enumerate(sample_data.items(), 1):
            info_print(f"\næ ·æœ¬{sample_idx} (è¡Œ {row_idx + 1}):")
            info_print(f"ç”¨æˆ·query: {data['user_input']}")
            info_print()
            
            # æ‰“å°ä¸å«æœ‰ç›¸å…³ä¿¡æ¯çš„åˆ†å—
            if data['irrelevant_chunks']:
                info_print(f"1. ä¸å«æœ‰ç›¸å…³ä¿¡æ¯çš„åˆ†å— ({len(data['irrelevant_chunks'])}ä¸ª):")
                for i, chunk_info in enumerate(data['irrelevant_chunks'], 1):
                    info_print(f"   {i}. æ£€ç´¢åˆ†å—: {chunk_info['retrieved_chunk'][:150]}...")
                    info_print(f"      ç›¸å…³æ€§åˆ†æ•°: {chunk_info['max_relevance']:.4f} ({self._get_relevance_level(chunk_info['max_relevance'])})")
                info_print()
            else:
                info_print("1. ä¸å«æœ‰ç›¸å…³ä¿¡æ¯çš„åˆ†å—: æ— ")
                info_print()
            
            # æ‰“å°æœªå¬å›åˆ†å—
            if data['missed_chunks']:
                info_print(f"2. æœªå¬å›åˆ†å— ({len(data['missed_chunks'])}ä¸ª):")
                for i, chunk_info in enumerate(data['missed_chunks'], 1):
                    info_print(f"   {i}. å‚è€ƒåˆ†å—: {chunk_info['reference_chunk'][:150]}...")
                    info_print(f"      ç›¸å…³æ€§åˆ†æ•°: {chunk_info['max_relevance']:.4f} ({self._get_relevance_level(chunk_info['max_relevance'])})")
                info_print()
            else:
                info_print("2. æœªå¬å›åˆ†å—: æ— ")
                info_print()
            
            info_print("-" * 60)
    
    def print_summary_metrics(self, results: Dict[str, Any]):
        """
        æ‰“å°æ±‡æ€»ä¿¡æ¯
        
        Args:
            results: è¯„ä¼°ç»“æœ
        """
        info_print("\n" + "=" * 80)
        info_print("ğŸ“ˆ æ±‡æ€»ä¿¡æ¯")
        info_print("=" * 80)
        
        info_print("ğŸ“‹ è¯„ä¼°æŒ‡æ ‡å®šä¹‰:")
        info_print("  â€¢ Precision = å®Œæ•´å«æœ‰ç›¸å…³ä¿¡æ¯çš„åˆ†å—æ•° / retrieved_contextsåˆ†å—æ•°")
        info_print("  â€¢ Recall = å®Œæ•´å«æœ‰ç›¸å…³ä¿¡æ¯çš„åˆ†å—æ•° / reference_contextsåˆ†å—æ•°")
        similarity_threshold = float(os.getenv("SIMILARITY_THRESHOLD", "0.5"))
        info_print(f"  â€¢ ç›¸å…³æ€§åˆ¤æ–­: æ£€ç´¢åˆ†å—ä¸å‚è€ƒåˆ†å—çš„è¯­ä¹‰ç›¸ä¼¼åº¦ > {similarity_threshold}")
        info_print()
        
        info_print("ğŸ“Š è¯„ä¼°ç»“æœ:")
        info_print(f"1. Precision: {results['avg_precision']:.4f} ({results['avg_precision']*100:.1f}%)")
        info_print(f"2. Recall: {results['avg_recall']:.4f} ({results['avg_recall']*100:.1f}%)")
        info_print(f"3. F1åˆ†æ•°: {results['avg_f1']:.4f} ({results['avg_f1']*100:.1f}%)")
        
        info_print(f"\nğŸ“Š ç»Ÿè®¡ä¿¡æ¯:")
        info_print(f"  â€¢ è¯„ä¼°æ ·æœ¬æ•°: {len(results['precision_scores'])} ä¸ª")
        info_print(f"  â€¢ ä¸ç›¸å…³æ£€ç´¢åˆ†å—æ€»æ•°: {len(results['irrelevant_chunks'])} ä¸ª")
        info_print(f"  â€¢ æœªå¬å›å‚è€ƒåˆ†å—æ€»æ•°: {len(results['missed_chunks'])} ä¸ª")
        
        # ç›¸å…³æ€§åˆ†å¸ƒç»Ÿè®¡
        irrelevant_scores = [chunk['max_relevance'] for chunk in results['irrelevant_chunks']]
        missed_scores = [chunk['max_relevance'] for chunk in results['missed_chunks']]
        
        if irrelevant_scores:
            info_print(f"\nğŸš« ä¸ç›¸å…³æ£€ç´¢åˆ†å—ç›¸å…³æ€§åˆ†å¸ƒ:")
            info_print(f"  â€¢ å¹³å‡åˆ†æ•°: {np.mean(irrelevant_scores):.4f}")
            info_print(f"  â€¢ æœ€é«˜åˆ†æ•°: {np.max(irrelevant_scores):.4f}")
            info_print(f"  â€¢ æœ€ä½åˆ†æ•°: {np.min(irrelevant_scores):.4f}")
        
        if missed_scores:
            info_print(f"\nâŒ æœªå¬å›å‚è€ƒåˆ†å—ç›¸å…³æ€§åˆ†å¸ƒ:")
            info_print(f"  â€¢ å¹³å‡åˆ†æ•°: {np.mean(missed_scores):.4f}")
            info_print(f"  â€¢ æœ€é«˜åˆ†æ•°: {np.max(missed_scores):.4f}")
            info_print(f"  â€¢ æœ€ä½åˆ†æ•°: {np.min(missed_scores):.4f}")
    
    def _get_relevance_level(self, score: float) -> str:
        """
        æ ¹æ®åˆ†æ•°è·å–ç›¸å…³æ€§ç­‰çº§
        
        Args:
            score: ç›¸å…³æ€§åˆ†æ•°
            
        Returns:
            str: ç›¸å…³æ€§ç­‰çº§æè¿°
        """
        if score >= 1.0:
            return self.relevance_thresholds[1.0000]
        elif score >= 0.75:
            return self.relevance_thresholds[0.7500]
        elif score >= 0.5:
            return self.relevance_thresholds[0.5000]
        elif score >= 0.25:
            return self.relevance_thresholds[0.2500]
        else:
            return self.relevance_thresholds[0.0000]
    





    
    def print_evaluation_summary(self, results: Dict[str, Any]):
        """
        æ‰“å°è¯„ä¼°æ€»ç»“ï¼ˆå·²å¼ƒç”¨ï¼Œä½¿ç”¨print_summary_metricså’Œprint_sample_analysisæ›¿ä»£ï¼‰
        
        Args:
            results: è¯„ä¼°ç»“æœ
        """
        info_print("\n" + "=" * 80)
        info_print("ğŸ“Š BM25è¯„ä¼°æ€»ç»“")
        info_print("=" * 80)
        
        info_print(f"ğŸ“ˆ å¹³å‡Precision: {results['avg_precision']:.4f} ({results['avg_precision']*100:.1f}%)")
        info_print(f"ğŸ“ˆ å¹³å‡Recall: {results['avg_recall']:.4f} ({results['avg_recall']*100:.1f}%)")
        info_print(f"ğŸ“ˆ å¹³å‡F1åˆ†æ•°: {results['avg_f1']:.4f} ({results['avg_f1']*100:.1f}%)")
        
        info_print(f"\nğŸ“Š è¯¦ç»†ç»Ÿè®¡:")
        info_print(f"  â€¢ ä¸ç›¸å…³çš„æ£€ç´¢åˆ†å—: {len(results['irrelevant_chunks'])} ä¸ª")
        info_print(f"  â€¢ æœªå¬å›çš„å‚è€ƒåˆ†å—: {len(results['missed_chunks'])} ä¸ª")
        info_print(f"  â€¢ è¯„ä¼°æ ·æœ¬æ•°: {len(results['precision_scores'])} ä¸ª")
    
    def run_evaluation(self) -> Dict[str, Any]:
        """
        è¿è¡Œå®Œæ•´çš„BM25è¯„ä¼°
        
        Returns:
            Dict[str, Any]: è¯„ä¼°ç»“æœ
        """
        info_print("ğŸš€ å¼€å§‹BM25 RAGè¯„ä¼°")
        info_print("=" * 60)
        
        try:
            # 1. åŠ è½½æ•°æ®
            df = self.load_and_process_data()
            if df is None:
                return {"error": "æ•°æ®åŠ è½½å¤±è´¥"}
            
            # 2. è¿è¡Œè¯„ä¼°
            results = self.evaluate_precision_recall(df)
            
            # 3. æ‰“å°ç»“æœ
            self.print_summary_metrics(results)
            self.print_sample_analysis(results)
            
            return results
            
        except Exception as e:
            info_print(f"âŒ è¯„ä¼°å¤±è´¥: {e}")
            import traceback
            traceback.print_exc()
            return {"error": str(e)}

def main():
    """ä¸»å‡½æ•°"""
    # åˆ›å»ºé…ç½®
    config = EvaluationConfig(
        api_key=os.getenv("QWEN_API_KEY", "dummy_key"),
        api_base=os.getenv("QWEN_API_BASE", "dummy_base")
    )
    
    # åˆ›å»ºè¯„ä¼°å™¨å¹¶è¿è¡Œè¯„ä¼°
    evaluator = BM25Evaluator(config)
    results = evaluator.run_evaluation()
    
    if "error" in results:
        info_print(f"âŒ è¯„ä¼°å¤±è´¥: {results['error']}")
    else:
        info_print(f"\nğŸ‰ BM25è¯„ä¼°æˆåŠŸå®Œæˆï¼")

def find_relevant_chunks(query: str, chunks: List[str], max_chunks: int = 10, threshold: float = -10.0) -> List[Tuple[str, float]]:
    """
    ä½¿ç”¨BM25ç®—æ³•æŸ¥æ‰¾ä¸æŸ¥è¯¢ç›¸å…³çš„åˆ†å—
    
    Args:
        query: æŸ¥è¯¢æ–‡æœ¬
        chunks: åˆ†å—åˆ—è¡¨
        max_chunks: æœ€å¤§è¿”å›åˆ†å—æ•°
        threshold: ç›¸å…³æ€§é˜ˆå€¼
        
    Returns:
        List[Tuple[str, float]]: ç›¸å…³åˆ†å—åˆ—è¡¨ï¼Œæ¯ä¸ªå…ƒç´ ä¸º(åˆ†å—å†…å®¹, ç›¸å…³æ€§åˆ†æ•°)
    """
    if not chunks or not query:
        return []
    
    # åˆ›å»ºBM25å®ä¾‹
    bm25 = BM25()
    bm25.fit(chunks)
    
    # è®¡ç®—æ‰€æœ‰åˆ†å—çš„BM25åˆ†æ•°
    scores = bm25.get_scores(query)
    
    # åˆ›å»º(åˆ†å—, åˆ†æ•°)å¯¹å¹¶æ’åº
    chunk_scores = [(chunks[i], scores[i]) for i in range(len(chunks))]
    chunk_scores.sort(key=lambda x: x[1], reverse=True)
    
    # è¿‡æ»¤æ‰ä½äºé˜ˆå€¼çš„åˆ†å—
    relevant_chunks = [(chunk, score) for chunk, score in chunk_scores if score > threshold]
    
    # è¿”å›å‰max_chunksä¸ªæœ€ç›¸å…³çš„åˆ†å—
    return relevant_chunks[:max_chunks]

def is_chunk_relevant(query: str, chunk: str, threshold: float = -10.0) -> Tuple[bool, float]:
    """
    åˆ¤æ–­å•ä¸ªåˆ†å—æ˜¯å¦ä¸æŸ¥è¯¢ç›¸å…³
    
    Args:
        query: æŸ¥è¯¢æ–‡æœ¬
        chunk: åˆ†å—å†…å®¹
        threshold: ç›¸å…³æ€§é˜ˆå€¼
        
    Returns:
        Tuple[bool, float]: (æ˜¯å¦ç›¸å…³, ç›¸å…³æ€§åˆ†æ•°)
    """
    if not chunk or not query:
        return False, 0.0
    
    # åˆ›å»ºBM25å®ä¾‹
    bm25 = BM25()
    bm25.fit([chunk])
    
    # è®¡ç®—BM25åˆ†æ•°
    score = bm25.score(query, 0)
    
    return score > threshold, score

if __name__ == "__main__":
    main()
