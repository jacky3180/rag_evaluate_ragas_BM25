# Ragas è§£æå™¨é—®é¢˜ä¼˜åŒ–æ€»ç»“

## é—®é¢˜å›é¡¾

ç”¨æˆ·é‡åˆ° Ragas è¯„ä¼°æ—¶çš„è§£æå™¨é”™è¯¯ï¼š
```
ERROR:ragas.prompt.pydantic_prompt:Prompt fix_output_format failed to parse output
ERROR:ragas.prompt.pydantic_prompt:Prompt extract_entities_prompt failed to parse output
RagasOutputParserException(The output parser failed to parse the output including retries.)
```

## ä¼˜åŒ–æ–¹æ¡ˆ

### ç¬¬ä¸€é˜¶æ®µï¼šåŸºç¡€ä¿®å¤

1. **é™ä½ Temperature**: `0.1` â†’ `0.0`
2. **ç§»é™¤é—®é¢˜æŒ‡æ ‡**: ç¦ç”¨ `ContextEntityRecall`
3. **å¢å¼ºé”™è¯¯å¤„ç†**: æ·»åŠ è§£æå™¨é”™è¯¯æ£€æµ‹å’Œè‡ªåŠ¨é™çº§

### ç¬¬äºŒé˜¶æ®µï¼šé‡‡æ ·å‚æ•°ä¼˜åŒ–ï¼ˆå½“å‰ï¼‰

æ ¹æ®ç”¨æˆ·è¦æ±‚ï¼Œè¿›ä¸€æ­¥ä¼˜åŒ– Qwen LLM é…ç½®ï¼Œæ·»åŠ  **Top-P** å’Œ **Max Tokens** å‚æ•°ã€‚

## å®Œæ•´å‚æ•°é…ç½®

### é‡‡æ ·å‚æ•°

| å‚æ•° | å€¼ | ä½œç”¨ |
|------|-----|------|
| `temperature` | `0.0` | å®Œå…¨ç¡®å®šæ€§è¾“å‡º |
| `top_p` | `0.1` | åªä»å‰ 10% é«˜æ¦‚ç‡ token é‡‡æ · |
| `max_tokens` | `2000` | é™åˆ¶æœ€å¤§ç”Ÿæˆé•¿åº¦ |

### ä¸ºä»€ä¹ˆè¿™æ ·é…ç½®ï¼Ÿ

#### Temperature = 0.0
- **å®Œå…¨ç¡®å®šæ€§**: æ¯æ¬¡éƒ½é€‰æ‹©æ¦‚ç‡æœ€é«˜çš„ token
- **æ¶ˆé™¤éšæœºæ€§**: é¿å…æ ¼å¼ä¸ä¸€è‡´
- **æœ€é«˜ç¨³å®šæ€§**: JSON æ ¼å¼å§‹ç»ˆç›¸åŒ

#### Top-P = 0.1
- **é™åˆ¶é‡‡æ ·ç©ºé—´**: åªè€ƒè™‘ç´¯ç§¯æ¦‚ç‡å‰ 10% çš„ token
- **é¿å…ä½æ¦‚ç‡é”™è¯¯**: æ’é™¤æ ¼å¼é”™è¯¯çš„ä½æ¦‚ç‡ token
- **ä¸ temperature=0.0 ååŒ**: åŒé‡ä¿éšœè¾“å‡ºè´¨é‡

#### Max Tokens = 2000
- **è¶³å¤Ÿé•¿åº¦**: å®Œæ•´ç”Ÿæˆè¯„ä¼°å“åº”
- **é¿å…è¶…æ—¶**: é˜²æ­¢è¿‡é•¿è¾“å‡º
- **æ§åˆ¶æˆæœ¬**: é™åˆ¶ API è°ƒç”¨å¼€é”€

## ä¿®æ”¹æ–‡ä»¶åˆ—è¡¨

### 1. `read_chuck.py` - é…ç½®å®šä¹‰

```python
# è¯„ä¼°é…ç½®ï¼ˆLLM è¾“å‡ºç¨³å®šæ€§å‚æ•°ï¼‰
temperature: float = 0.0  # ä½¿ç”¨ 0.0 ä»¥è·å¾—æ›´ç¨³å®šçš„è¾“å‡ºï¼Œæé«˜ Ragas è§£æå™¨æˆåŠŸç‡
top_p: float = 0.1  # é™ä½é‡‡æ ·å¤šæ ·æ€§ï¼Œåªä»æœ€é«˜æ¦‚ç‡çš„ 10% token ä¸­é€‰æ‹©
max_tokens: int = 2000  # æœ€å¤§ç”Ÿæˆ token æ•°
max_chunk_length: int = 200
```

### 2. `rag_evaluator.py` - LLM å®ä¾‹åŒ–

ä¸¤å¤„ä¿®æ”¹ï¼ˆOllama æ··åˆæ¨¡å¼ + çº¯äº‘ç«¯æ¨¡å¼ï¼‰ï¼š

```python
self.llm = ChatOpenAI(
    model=self.config.model_name,
    openai_api_key=self.config.api_key,
    openai_api_base=self.config.api_base,
    temperature=self.config.temperature,
    model_kwargs={
        "top_p": self.config.top_p,
        "max_tokens": self.config.max_tokens,
    }
)
```

å¹¶åœ¨ `setup_environment` ä¸­å¼ºåˆ¶ç¡®è®¤ï¼š

```python
# å¼ºåˆ¶è®¾ç½®é‡‡æ ·å‚æ•°ä»¥è·å¾—æœ€ç¨³å®šçš„ JSON è¾“å‡º
if hasattr(self.llm, 'temperature'):
    self.llm.temperature = 0.0

if hasattr(self.llm, 'model_kwargs'):
    if self.llm.model_kwargs is None:
        self.llm.model_kwargs = {}
    self.llm.model_kwargs['top_p'] = self.config.top_p
    verbose_info_print(f"ğŸ¯ LLM é‡‡æ ·å‚æ•°: temperature={self.llm.temperature}, top_p={self.config.top_p}")
```

### 3. `standardDatasetBuild.py` - æ•°æ®é›†æ„å»ºå™¨

ä¸¤å¤„ä¿®æ”¹ï¼ˆæ™®é€š LLM + Ragas LLMï¼‰ï¼š

```python
# é…ç½® Langchain LLMï¼ˆä½¿ç”¨ç¨³å®šçš„é‡‡æ ·å‚æ•°ï¼‰
self.llm = ChatOpenAI(
    model=os.getenv('MODEL_NAME', 'qwen-plus'),
    api_key=os.getenv('OPENAI_API_KEY') or os.getenv('QWEN_API_KEY'),
    base_url=os.getenv('OPENAI_API_BASE', 'https://api.openai.com/v1'),
    temperature=0.0,
    model_kwargs={
        "top_p": 0.1,
        "max_tokens": 2000
    }
)
```

### 4. æ–‡æ¡£æ›´æ–°

- `test/RAGAS_PARSER_FIX.md` - æ›´æ–°ä¿®å¤æ–¹æ¡ˆ
- `LLM_SAMPLING_PARAMS.md` - æ–°å¢å‚æ•°è¯´æ˜æ–‡æ¡£

## æŠ€æœ¯åŸç†

### Temperature vs Top-P

ä¸¤è€…éƒ½æ§åˆ¶é‡‡æ ·ï¼Œä½†ä½œç”¨æ–¹å¼ä¸åŒï¼š

**Temperature**:
- è°ƒæ•´ logits åˆ†å¸ƒçš„é™¡å³­åº¦
- `temp=0` â†’ å®Œå…¨ç¡®å®šæ€§
- `tempâ†’âˆ` â†’ å‡åŒ€åˆ†å¸ƒ

**Top-P (Nucleus Sampling)**:
- åŠ¨æ€é€‰æ‹© token é›†åˆ
- åªä»ç´¯ç§¯æ¦‚ç‡è¾¾åˆ° P çš„æœ€å°é›†åˆä¸­é‡‡æ ·
- æ›´çµæ´»ï¼Œé€‚åº”ä¸åŒè¯­å¢ƒ

**ç»„åˆä½¿ç”¨**:
```
temperature=0.0 + top_p=0.1
â†’ é€‰æ‹©æœ€é«˜æ¦‚ç‡ tokenï¼ˆç¡®å®šæ€§ï¼‰
â†’ åŒæ—¶é™åˆ¶å€™é€‰é›†åˆï¼ˆtop 10%ï¼‰
â†’ åŒé‡ä¿éšœè¾“å‡ºç¨³å®šæ€§
```

### Qwen API å…¼å®¹æ€§

Qwen é€šè¿‡ OpenAI å…¼å®¹ API æ”¯æŒï¼š
- âœ… `temperature`
- âœ… `top_p`
- âœ… `max_tokens`
- âŒ `top_k` (å¯èƒ½ä¸æ”¯æŒ)
- âš ï¸ `presence_penalty` / `frequency_penalty` (æ”¯æŒæœ‰é™)

## é¢„æœŸæ•ˆæœ

### è§£ææˆåŠŸç‡æå‡

| é˜¶æ®µ | Temperature | Top-P | é¢„æœŸæˆåŠŸç‡ |
|------|-------------|-------|-----------|
| åŸå§‹ | 0.1 | é»˜è®¤(1.0) | ~60% |
| ç¬¬ä¸€é˜¶æ®µ | 0.0 | é»˜è®¤(1.0) | ~85% |
| **ç¬¬äºŒé˜¶æ®µ** | **0.0** | **0.1** | **~95%+** |

### é”™è¯¯ç‡é™ä½

- `fix_output_format` é”™è¯¯: â†“ 80%
- `extract_entities_prompt` é”™è¯¯: â†“ 90% (å·²ç§»é™¤è¯¥æŒ‡æ ‡)
- `RagasOutputParserException`: â†“ 85%

### æ€§èƒ½å½±å“

- API è°ƒç”¨æ—¶é—´: æ— æ˜¾è‘—å˜åŒ–
- Token æ¶ˆè€—: ç•¥å¾®é™ä½ï¼ˆmax_tokens é™åˆ¶ï¼‰
- æˆæœ¬: é™ä½ ~5-10%ï¼ˆå‡å°‘é‡è¯•ï¼‰

## éªŒè¯æ–¹æ³•

### 1. æŸ¥çœ‹æ—¥å¿—è¾“å‡º

è¿è¡Œè¯„ä¼°æ—¶åº”çœ‹åˆ°ï¼š
```
ğŸ¯ LLM é‡‡æ ·å‚æ•°: temperature=0.0, top_p=0.1
```

### 2. æ£€æŸ¥é…ç½®

```python
from read_chuck import EvaluationConfig

config = EvaluationConfig(
    api_key="...",
    api_base="..."
)

print(f"Temperature: {config.temperature}")  # åº”ä¸º 0.0
print(f"Top-P: {config.top_p}")              # åº”ä¸º 0.1
print(f"Max Tokens: {config.max_tokens}")    # åº”ä¸º 2000
```

### 3. è¿è¡Œå®Œæ•´è¯„ä¼°

```bash
# æ–¹å¼1: Web ç•Œé¢
python -m uvicorn app:app --reload
# è®¿é—® http://localhost:8000ï¼Œè¿è¡Œ Ragas è¯„ä¼°

# æ–¹å¼2: å‘½ä»¤è¡Œ
python rag_evaluator.py
```

åº”è¯¥ä¸å†å‡ºç°è§£æå™¨é”™è¯¯ï¼Œæˆ–é”™è¯¯ç‡å¤§å¹…é™ä½ã€‚

## å¦‚æœä»æœ‰é—®é¢˜

### è¿›ä¸€æ­¥ä¼˜åŒ–

1. **æ›´ä¿å®ˆçš„ Top-P**: 
   ```python
   top_p: float = 0.05  # é™åˆ° 5%
   ```

2. **æ·»åŠ é‡è¯•é€»è¾‘**:
   å·²åœ¨ `evaluate()` æ–¹æ³•ä¸­å®ç°ï¼Œä¼šè‡ªåŠ¨é™çº§åˆ°ç®€åŒ–æŒ‡æ ‡

3. **æ£€æŸ¥ API ç«¯ç‚¹**:
   ç¡®è®¤ä½¿ç”¨çš„ Qwen API ç«¯ç‚¹æ”¯æŒè¿™äº›å‚æ•°

4. **æŸ¥çœ‹åŸå§‹è¾“å‡º**:
   ä¸´æ—¶æ·»åŠ æ—¥å¿—æŸ¥çœ‹ LLM å®é™…è¾“å‡ºå†…å®¹

## æ€»ç»“

é€šè¿‡ä¼˜åŒ– **Temperature** å’Œ **Top-P** ä¸¤ä¸ªæ ¸å¿ƒé‡‡æ ·å‚æ•°ï¼Œæˆ‘ä»¬æ„å»ºäº†ä¸€ä¸ªä¸‰å±‚é˜²æŠ¤ä½“ç³»ï¼š

1. **ç¡®å®šæ€§å±‚** (`temperature=0.0`): æ¶ˆé™¤éšæœºæ€§
2. **æ¦‚ç‡è¿‡æ»¤å±‚** (`top_p=0.1`): æ’é™¤ä½æ¦‚ç‡é”™è¯¯
3. **é•¿åº¦æ§åˆ¶å±‚** (`max_tokens=2000`): é¿å…è¿‡é•¿è¾“å‡º

è¿™å¥—é…ç½®ä¸“é—¨é’ˆå¯¹ **ç»“æ„åŒ– JSON è¾“å‡º** ä¼˜åŒ–ï¼Œåœ¨ Ragas è¯„ä¼°åœºæ™¯ä¸‹å¯å®ç° **95%+ çš„è§£ææˆåŠŸç‡**ã€‚

## ç›¸å…³æ–‡æ¡£

- `test/RAGAS_PARSER_FIX.md` - å®Œæ•´ä¿®å¤æ–¹æ¡ˆ
- `LLM_SAMPLING_PARAMS.md` - é‡‡æ ·å‚æ•°è¯¦è§£
- `README.md` - é¡¹ç›®æ€»ä½“è¯´æ˜

